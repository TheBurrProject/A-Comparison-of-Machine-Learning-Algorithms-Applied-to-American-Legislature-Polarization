{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Corinne Kleinman and Gabriel Mersy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed for reproducible results\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>percent_with_disability</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "      <th>senate_polarization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.721508</td>\n",
       "      <td>14.8</td>\n",
       "      <td>42849</td>\n",
       "      <td>15.9</td>\n",
       "      <td>14928</td>\n",
       "      <td>57</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>63.450</td>\n",
       "      <td>0.929447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.882121</td>\n",
       "      <td>18.2</td>\n",
       "      <td>72237</td>\n",
       "      <td>11.1</td>\n",
       "      <td>7081</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>69.588</td>\n",
       "      <td>0.880214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.841647</td>\n",
       "      <td>17.1</td>\n",
       "      <td>48510</td>\n",
       "      <td>12.3</td>\n",
       "      <td>44711</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>68.827</td>\n",
       "      <td>1.439661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.302302</td>\n",
       "      <td>13.4</td>\n",
       "      <td>40511</td>\n",
       "      <td>17.1</td>\n",
       "      <td>9377</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>64.120</td>\n",
       "      <td>0.554589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.750678</td>\n",
       "      <td>19.5</td>\n",
       "      <td>60190</td>\n",
       "      <td>10.6</td>\n",
       "      <td>291448</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>75.241</td>\n",
       "      <td>1.538937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "0                           64.721508                                  14.8   \n",
       "1                           79.882121                                  18.2   \n",
       "2                           74.841647                                  17.1   \n",
       "3                           62.302302                                  13.4   \n",
       "4                           78.750678                                  19.5   \n",
       "\n",
       "   household_median_income  percent_with_disability  \\\n",
       "0                    42849                     15.9   \n",
       "1                    72237                     11.1   \n",
       "2                    48510                     12.3   \n",
       "3                    40511                     17.1   \n",
       "4                    60190                     10.6   \n",
       "\n",
       "   total_moved_to_US_from_abroad  Percent_Very_Religious  \\\n",
       "0                          14928                      57   \n",
       "1                           7081                      38   \n",
       "2                          44711                      36   \n",
       "3                           9377                      51   \n",
       "4                         291448                      34   \n",
       "\n",
       "   Percent_Nonreligious  mass_shootings  believes_climate_change  \\\n",
       "0                    15               1                   63.450   \n",
       "1                    35               0                   69.588   \n",
       "2                    35               1                   68.827   \n",
       "3                    19               0                   64.120   \n",
       "4                    37               4                   75.241   \n",
       "\n",
       "   senate_polarization  \n",
       "0             0.929447  \n",
       "1             0.880214  \n",
       "2             1.439661  \n",
       "3             0.554589  \n",
       "4             1.538937  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in data\n",
    "dat = pd.read_csv('./demographicDataCleanMerged11.csv', sep = ',')\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train, test = sklearn.model_selection.train_test_split(dat, test_size = 0.2, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popping the labels under consideration\n",
    "train_label = train.pop('senate_polarization')\n",
    "test_label = test.pop('senate_polarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>percent_with_disability</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-0.401570</td>\n",
       "      <td>1.705499</td>\n",
       "      <td>1.072746</td>\n",
       "      <td>-1.031287</td>\n",
       "      <td>-0.159451</td>\n",
       "      <td>-0.049657</td>\n",
       "      <td>-0.152386</td>\n",
       "      <td>-0.017854</td>\n",
       "      <td>0.205642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.020013</td>\n",
       "      <td>-0.070592</td>\n",
       "      <td>1.669380</td>\n",
       "      <td>-0.937946</td>\n",
       "      <td>-0.627237</td>\n",
       "      <td>-1.390386</td>\n",
       "      <td>1.988453</td>\n",
       "      <td>-0.532063</td>\n",
       "      <td>0.368208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.024416</td>\n",
       "      <td>-0.146171</td>\n",
       "      <td>-0.179944</td>\n",
       "      <td>-0.704594</td>\n",
       "      <td>3.193395</td>\n",
       "      <td>0.062071</td>\n",
       "      <td>0.115219</td>\n",
       "      <td>-0.017854</td>\n",
       "      <td>0.199704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.381751</td>\n",
       "      <td>0.118354</td>\n",
       "      <td>-0.698908</td>\n",
       "      <td>0.322156</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>1.179345</td>\n",
       "      <td>-1.133604</td>\n",
       "      <td>2.038979</td>\n",
       "      <td>0.205889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-0.793231</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>0.196541</td>\n",
       "      <td>-0.564583</td>\n",
       "      <td>-0.343281</td>\n",
       "      <td>-0.049657</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>-0.532063</td>\n",
       "      <td>0.006208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "153                           -0.401570                              1.705499   \n",
       "47                             1.020013                             -0.070592   \n",
       "84                            -0.024416                             -0.146171   \n",
       "117                            0.381751                              0.118354   \n",
       "178                           -0.793231                              0.269510   \n",
       "\n",
       "     household_median_income  percent_with_disability  \\\n",
       "153                 1.072746                -1.031287   \n",
       "47                  1.669380                -0.937946   \n",
       "84                 -0.179944                -0.704594   \n",
       "117                -0.698908                 0.322156   \n",
       "178                 0.196541                -0.564583   \n",
       "\n",
       "     total_moved_to_US_from_abroad  Percent_Very_Religious  \\\n",
       "153                      -0.159451               -0.049657   \n",
       "47                       -0.627237               -1.390386   \n",
       "84                        3.193395                0.062071   \n",
       "117                       0.131939                1.179345   \n",
       "178                      -0.343281               -0.049657   \n",
       "\n",
       "     Percent_Nonreligious  mass_shootings  believes_climate_change  \n",
       "153             -0.152386       -0.017854                 0.205642  \n",
       "47               1.988453       -0.532063                 0.368208  \n",
       "84               0.115219       -0.017854                 0.199704  \n",
       "117             -1.133604        2.038979                 0.205889  \n",
       "178              0.026017       -0.532063                 0.006208  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing train features\n",
    "stats = train.describe()\n",
    "stats = stats.transpose()\n",
    "\n",
    "def normalize(val):\n",
    "    return((val - stats['mean']) / (stats['std']))\n",
    "\n",
    "normTrainFeatures = normalize(train)\n",
    "\n",
    "normTrainFeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building feed forward neural network model with single hidden layer\n",
    "def buildModel():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(5, input_dim = 9)) \n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss='mse', \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 48 samples\n",
      "Epoch 1/500\n",
      "96/96 [==============================] - 0s 2ms/sample - loss: 1.0510 - rmse: 1.0252 - val_loss: 1.3105 - val_rmse: 1.1448\n",
      "Epoch 2/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 1.0239 - rmse: 1.0119 - val_loss: 1.2727 - val_rmse: 1.1281\n",
      "Epoch 3/500\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 0.9945 - rmse: 0.9973 - val_loss: 1.2363 - val_rmse: 1.1119\n",
      "Epoch 4/500\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 0.9676 - rmse: 0.9837 - val_loss: 1.2010 - val_rmse: 1.0959\n",
      "Epoch 5/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.9410 - rmse: 0.9701 - val_loss: 1.1672 - val_rmse: 1.0804\n",
      "Epoch 6/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.9157 - rmse: 0.9569 - val_loss: 1.1342 - val_rmse: 1.0650\n",
      "Epoch 7/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.8905 - rmse: 0.9437 - val_loss: 1.1020 - val_rmse: 1.0497\n",
      "Epoch 8/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.8641 - rmse: 0.9296 - val_loss: 1.0714 - val_rmse: 1.0351\n",
      "Epoch 9/500\n",
      "96/96 [==============================] - 0s 139us/sample - loss: 0.8425 - rmse: 0.9179 - val_loss: 1.0417 - val_rmse: 1.0206\n",
      "Epoch 10/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.8194 - rmse: 0.9052 - val_loss: 1.0133 - val_rmse: 1.0067\n",
      "Epoch 11/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.7999 - rmse: 0.8943 - val_loss: 0.9858 - val_rmse: 0.9929\n",
      "Epoch 12/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.7776 - rmse: 0.8818 - val_loss: 0.9600 - val_rmse: 0.9798\n",
      "Epoch 13/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.7571 - rmse: 0.8701 - val_loss: 0.9355 - val_rmse: 0.9672\n",
      "Epoch 14/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.7405 - rmse: 0.8605 - val_loss: 0.9116 - val_rmse: 0.9548\n",
      "Epoch 15/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.7218 - rmse: 0.8496 - val_loss: 0.8888 - val_rmse: 0.9428\n",
      "Epoch 16/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.7038 - rmse: 0.8389 - val_loss: 0.8664 - val_rmse: 0.9308\n",
      "Epoch 17/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.6879 - rmse: 0.8294 - val_loss: 0.8447 - val_rmse: 0.9191\n",
      "Epoch 18/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.6706 - rmse: 0.8189 - val_loss: 0.8241 - val_rmse: 0.9078\n",
      "Epoch 19/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.6556 - rmse: 0.8097 - val_loss: 0.8039 - val_rmse: 0.8966\n",
      "Epoch 20/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.6396 - rmse: 0.7997 - val_loss: 0.7843 - val_rmse: 0.8856\n",
      "Epoch 21/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.6260 - rmse: 0.7912 - val_loss: 0.7637 - val_rmse: 0.8739\n",
      "Epoch 22/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.6114 - rmse: 0.7819 - val_loss: 0.7440 - val_rmse: 0.8625\n",
      "Epoch 23/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.5983 - rmse: 0.7735 - val_loss: 0.7243 - val_rmse: 0.8511\n",
      "Epoch 24/500\n",
      "96/96 [==============================] - 0s 111us/sample - loss: 0.5846 - rmse: 0.7646 - val_loss: 0.7055 - val_rmse: 0.8400\n",
      "Epoch 25/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.5715 - rmse: 0.7560 - val_loss: 0.6872 - val_rmse: 0.8290\n",
      "Epoch 26/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.5576 - rmse: 0.7467 - val_loss: 0.6697 - val_rmse: 0.8184\n",
      "Epoch 27/500\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 0.5457 - rmse: 0.7387 - val_loss: 0.6524 - val_rmse: 0.8077\n",
      "Epoch 28/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.5342 - rmse: 0.7309 - val_loss: 0.6352 - val_rmse: 0.7970\n",
      "Epoch 29/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.5217 - rmse: 0.7223 - val_loss: 0.6186 - val_rmse: 0.7865\n",
      "Epoch 30/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.5099 - rmse: 0.7141 - val_loss: 0.6030 - val_rmse: 0.7765\n",
      "Epoch 31/500\n",
      "96/96 [==============================] - 0s 139us/sample - loss: 0.4995 - rmse: 0.7068 - val_loss: 0.5876 - val_rmse: 0.7665\n",
      "Epoch 32/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.4900 - rmse: 0.7000 - val_loss: 0.5726 - val_rmse: 0.7567\n",
      "Epoch 33/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.4787 - rmse: 0.6919 - val_loss: 0.5585 - val_rmse: 0.7473\n",
      "Epoch 34/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.4694 - rmse: 0.6851 - val_loss: 0.5449 - val_rmse: 0.7382\n",
      "Epoch 35/500\n",
      "96/96 [==============================] - 0s 109us/sample - loss: 0.4597 - rmse: 0.6780 - val_loss: 0.5318 - val_rmse: 0.7293\n",
      "Epoch 36/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.4507 - rmse: 0.6713 - val_loss: 0.5192 - val_rmse: 0.7205\n",
      "Epoch 37/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.4418 - rmse: 0.6647 - val_loss: 0.5070 - val_rmse: 0.7120\n",
      "Epoch 38/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.4334 - rmse: 0.6583 - val_loss: 0.4953 - val_rmse: 0.7038\n",
      "Epoch 39/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.4256 - rmse: 0.6523 - val_loss: 0.4840 - val_rmse: 0.6957\n",
      "Epoch 40/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.4176 - rmse: 0.6462 - val_loss: 0.4731 - val_rmse: 0.6878\n",
      "Epoch 41/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.4094 - rmse: 0.6399 - val_loss: 0.4628 - val_rmse: 0.6803\n",
      "Epoch 42/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.4025 - rmse: 0.6344 - val_loss: 0.4525 - val_rmse: 0.6727\n",
      "Epoch 43/500\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 0.3944 - rmse: 0.6280 - val_loss: 0.4428 - val_rmse: 0.6654\n",
      "Epoch 44/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.3871 - rmse: 0.6222 - val_loss: 0.4333 - val_rmse: 0.6583\n",
      "Epoch 45/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.3802 - rmse: 0.6166 - val_loss: 0.4239 - val_rmse: 0.6511\n",
      "Epoch 46/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.3733 - rmse: 0.6110 - val_loss: 0.4146 - val_rmse: 0.6439\n",
      "Epoch 47/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.3664 - rmse: 0.6053 - val_loss: 0.4055 - val_rmse: 0.6368\n",
      "Epoch 48/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.3598 - rmse: 0.5998 - val_loss: 0.3964 - val_rmse: 0.6296\n",
      "Epoch 49/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.3532 - rmse: 0.5943 - val_loss: 0.3875 - val_rmse: 0.6225\n",
      "Epoch 50/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.3472 - rmse: 0.5892 - val_loss: 0.3789 - val_rmse: 0.6156\n",
      "Epoch 51/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.3413 - rmse: 0.5842 - val_loss: 0.3705 - val_rmse: 0.6087\n",
      "Epoch 52/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.3355 - rmse: 0.5792 - val_loss: 0.3623 - val_rmse: 0.6019\n",
      "Epoch 53/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.3291 - rmse: 0.5737 - val_loss: 0.3548 - val_rmse: 0.5956\n",
      "Epoch 54/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.3243 - rmse: 0.5694 - val_loss: 0.3472 - val_rmse: 0.5892\n",
      "Epoch 55/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.3187 - rmse: 0.5645 - val_loss: 0.3402 - val_rmse: 0.5833\n",
      "Epoch 56/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.3140 - rmse: 0.5604 - val_loss: 0.3335 - val_rmse: 0.5775\n",
      "Epoch 57/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.3094 - rmse: 0.5562 - val_loss: 0.3271 - val_rmse: 0.5719\n",
      "Epoch 58/500\n",
      "96/96 [==============================] - 0s 122us/sample - loss: 0.3045 - rmse: 0.5518 - val_loss: 0.3211 - val_rmse: 0.5666\n",
      "Epoch 59/500\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 0.3002 - rmse: 0.5479 - val_loss: 0.3152 - val_rmse: 0.5614\n",
      "Epoch 60/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.2958 - rmse: 0.5439 - val_loss: 0.3098 - val_rmse: 0.5566\n",
      "Epoch 61/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.2917 - rmse: 0.5401 - val_loss: 0.3045 - val_rmse: 0.5518\n",
      "Epoch 62/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.2875 - rmse: 0.5362 - val_loss: 0.2996 - val_rmse: 0.5474\n",
      "Epoch 63/500\n",
      "96/96 [==============================] - 0s 108us/sample - loss: 0.2834 - rmse: 0.5324 - val_loss: 0.2950 - val_rmse: 0.5432\n",
      "Epoch 64/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.2798 - rmse: 0.5290 - val_loss: 0.2906 - val_rmse: 0.5391\n",
      "Epoch 65/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.2763 - rmse: 0.5256 - val_loss: 0.2864 - val_rmse: 0.5352\n",
      "Epoch 66/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.2727 - rmse: 0.5222 - val_loss: 0.2824 - val_rmse: 0.5314\n",
      "Epoch 67/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.2694 - rmse: 0.5190 - val_loss: 0.2785 - val_rmse: 0.5277\n",
      "Epoch 68/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.2662 - rmse: 0.5160 - val_loss: 0.2749 - val_rmse: 0.5243\n",
      "Epoch 69/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.2628 - rmse: 0.5126 - val_loss: 0.2715 - val_rmse: 0.5210\n",
      "Epoch 70/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.2601 - rmse: 0.5100 - val_loss: 0.2681 - val_rmse: 0.5178\n",
      "Epoch 71/500\n",
      "96/96 [==============================] - 0s 113us/sample - loss: 0.2571 - rmse: 0.5070 - val_loss: 0.2650 - val_rmse: 0.5147\n",
      "Epoch 72/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.2538 - rmse: 0.5038 - val_loss: 0.2621 - val_rmse: 0.5119\n",
      "Epoch 73/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.2512 - rmse: 0.5011 - val_loss: 0.2593 - val_rmse: 0.5092\n",
      "Epoch 74/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.2486 - rmse: 0.4986 - val_loss: 0.2566 - val_rmse: 0.5065\n",
      "Epoch 75/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.2458 - rmse: 0.4957 - val_loss: 0.2540 - val_rmse: 0.5040\n",
      "Epoch 76/500\n",
      "96/96 [==============================] - 0s 121us/sample - loss: 0.2432 - rmse: 0.4932 - val_loss: 0.2515 - val_rmse: 0.5014\n",
      "Epoch 77/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.2407 - rmse: 0.4906 - val_loss: 0.2491 - val_rmse: 0.4991\n",
      "Epoch 78/500\n",
      "96/96 [==============================] - 0s 129us/sample - loss: 0.2380 - rmse: 0.4878 - val_loss: 0.2468 - val_rmse: 0.4968\n",
      "Epoch 79/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.2356 - rmse: 0.4854 - val_loss: 0.2448 - val_rmse: 0.4948\n",
      "Epoch 80/500\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 0.2332 - rmse: 0.4829 - val_loss: 0.2426 - val_rmse: 0.4925\n",
      "Epoch 81/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.2309 - rmse: 0.4805 - val_loss: 0.2404 - val_rmse: 0.4904\n",
      "Epoch 82/500\n",
      "96/96 [==============================] - 0s 128us/sample - loss: 0.2286 - rmse: 0.4781 - val_loss: 0.2384 - val_rmse: 0.4883\n",
      "Epoch 83/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.2262 - rmse: 0.4756 - val_loss: 0.2364 - val_rmse: 0.4863\n",
      "Epoch 84/500\n",
      "96/96 [==============================] - 0s 151us/sample - loss: 0.2239 - rmse: 0.4732 - val_loss: 0.2345 - val_rmse: 0.4843\n",
      "Epoch 85/500\n",
      "96/96 [==============================] - 0s 134us/sample - loss: 0.2217 - rmse: 0.4708 - val_loss: 0.2327 - val_rmse: 0.4823\n",
      "Epoch 86/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.2196 - rmse: 0.4686 - val_loss: 0.2307 - val_rmse: 0.4803\n",
      "Epoch 87/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.2173 - rmse: 0.4662 - val_loss: 0.2289 - val_rmse: 0.4784\n",
      "Epoch 88/500\n",
      "96/96 [==============================] - 0s 117us/sample - loss: 0.2151 - rmse: 0.4638 - val_loss: 0.2272 - val_rmse: 0.4766\n",
      "Epoch 89/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.2129 - rmse: 0.4614 - val_loss: 0.2254 - val_rmse: 0.4747\n",
      "Epoch 90/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.2110 - rmse: 0.4593 - val_loss: 0.2236 - val_rmse: 0.4729\n",
      "Epoch 91/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.2089 - rmse: 0.4570 - val_loss: 0.2220 - val_rmse: 0.4712\n",
      "Epoch 92/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.2067 - rmse: 0.4546 - val_loss: 0.2205 - val_rmse: 0.4696\n",
      "Epoch 93/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.2048 - rmse: 0.4525 - val_loss: 0.2191 - val_rmse: 0.4681\n",
      "Epoch 94/500\n",
      "96/96 [==============================] - 0s 125us/sample - loss: 0.2027 - rmse: 0.4502 - val_loss: 0.2177 - val_rmse: 0.4666\n",
      "Epoch 95/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.2008 - rmse: 0.4481 - val_loss: 0.2163 - val_rmse: 0.4651\n",
      "Epoch 96/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1987 - rmse: 0.4458 - val_loss: 0.2149 - val_rmse: 0.4636\n",
      "Epoch 97/500\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 0.1969 - rmse: 0.4437 - val_loss: 0.2136 - val_rmse: 0.4622\n",
      "Epoch 98/500\n",
      "96/96 [==============================] - 0s 173us/sample - loss: 0.1950 - rmse: 0.4416 - val_loss: 0.2123 - val_rmse: 0.4607\n",
      "Epoch 99/500\n",
      "96/96 [==============================] - 0s 117us/sample - loss: 0.1931 - rmse: 0.4394 - val_loss: 0.2109 - val_rmse: 0.4593\n",
      "Epoch 100/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.1914 - rmse: 0.4375 - val_loss: 0.2097 - val_rmse: 0.4579\n",
      "Epoch 101/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.1895 - rmse: 0.4353 - val_loss: 0.2083 - val_rmse: 0.4564\n",
      "Epoch 102/500\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 0.1878 - rmse: 0.4334 - val_loss: 0.2070 - val_rmse: 0.4550\n",
      "Epoch 103/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.1859 - rmse: 0.4312 - val_loss: 0.2056 - val_rmse: 0.4535\n",
      "Epoch 104/500\n",
      "96/96 [==============================] - 0s 113us/sample - loss: 0.1842 - rmse: 0.4291 - val_loss: 0.2043 - val_rmse: 0.4520\n",
      "Epoch 105/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.1825 - rmse: 0.4272 - val_loss: 0.2031 - val_rmse: 0.4506\n",
      "Epoch 106/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.1808 - rmse: 0.4252 - val_loss: 0.2017 - val_rmse: 0.4491\n",
      "Epoch 107/500\n",
      "96/96 [==============================] - 0s 74us/sample - loss: 0.1791 - rmse: 0.4232 - val_loss: 0.2003 - val_rmse: 0.4476\n",
      "Epoch 108/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.1774 - rmse: 0.4211 - val_loss: 0.1990 - val_rmse: 0.4461\n",
      "Epoch 109/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.1757 - rmse: 0.4191 - val_loss: 0.1977 - val_rmse: 0.4446\n",
      "Epoch 110/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.1740 - rmse: 0.4171 - val_loss: 0.1963 - val_rmse: 0.4430\n",
      "Epoch 111/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1724 - rmse: 0.4152 - val_loss: 0.1949 - val_rmse: 0.4415\n",
      "Epoch 112/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.1707 - rmse: 0.4132 - val_loss: 0.1935 - val_rmse: 0.4399\n",
      "Epoch 113/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.1691 - rmse: 0.4113 - val_loss: 0.1922 - val_rmse: 0.4384\n",
      "Epoch 114/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.1675 - rmse: 0.4092 - val_loss: 0.1909 - val_rmse: 0.4369\n",
      "Epoch 115/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.1660 - rmse: 0.4074 - val_loss: 0.1896 - val_rmse: 0.4354\n",
      "Epoch 116/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.1645 - rmse: 0.4056 - val_loss: 0.1882 - val_rmse: 0.4338\n",
      "Epoch 117/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.1628 - rmse: 0.4035 - val_loss: 0.1870 - val_rmse: 0.4324\n",
      "Epoch 118/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1613 - rmse: 0.4017 - val_loss: 0.1856 - val_rmse: 0.4308\n",
      "Epoch 119/500\n",
      "96/96 [==============================] - 0s 109us/sample - loss: 0.1597 - rmse: 0.3996 - val_loss: 0.1843 - val_rmse: 0.4293\n",
      "Epoch 120/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1583 - rmse: 0.3978 - val_loss: 0.1830 - val_rmse: 0.4278\n",
      "Epoch 121/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.1568 - rmse: 0.3960 - val_loss: 0.1816 - val_rmse: 0.4262\n",
      "Epoch 122/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1552 - rmse: 0.3940 - val_loss: 0.1803 - val_rmse: 0.4246\n",
      "Epoch 123/500\n",
      "96/96 [==============================] - 0s 123us/sample - loss: 0.1538 - rmse: 0.3922 - val_loss: 0.1790 - val_rmse: 0.4231\n",
      "Epoch 124/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.1523 - rmse: 0.3903 - val_loss: 0.1777 - val_rmse: 0.4215\n",
      "Epoch 125/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.1509 - rmse: 0.3885 - val_loss: 0.1764 - val_rmse: 0.4200\n",
      "Epoch 126/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.1494 - rmse: 0.3865 - val_loss: 0.1750 - val_rmse: 0.4183\n",
      "Epoch 127/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.1480 - rmse: 0.3847 - val_loss: 0.1736 - val_rmse: 0.4166\n",
      "Epoch 128/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.1465 - rmse: 0.3828 - val_loss: 0.1722 - val_rmse: 0.4150\n",
      "Epoch 129/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.1451 - rmse: 0.3810 - val_loss: 0.1709 - val_rmse: 0.4134\n",
      "Epoch 130/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.1436 - rmse: 0.3790 - val_loss: 0.1696 - val_rmse: 0.4119\n",
      "Epoch 131/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.1423 - rmse: 0.3773 - val_loss: 0.1683 - val_rmse: 0.4102\n",
      "Epoch 132/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1410 - rmse: 0.3755 - val_loss: 0.1670 - val_rmse: 0.4087\n",
      "Epoch 133/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.1396 - rmse: 0.3736 - val_loss: 0.1658 - val_rmse: 0.4071\n",
      "Epoch 134/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1383 - rmse: 0.3718 - val_loss: 0.1644 - val_rmse: 0.4055\n",
      "Epoch 135/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.1369 - rmse: 0.3699 - val_loss: 0.1632 - val_rmse: 0.4039\n",
      "Epoch 136/500\n",
      "96/96 [==============================] - 0s 154us/sample - loss: 0.1355 - rmse: 0.3681 - val_loss: 0.1619 - val_rmse: 0.4024\n",
      "Epoch 137/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.1343 - rmse: 0.3664 - val_loss: 0.1606 - val_rmse: 0.4008\n",
      "Epoch 138/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.1329 - rmse: 0.3646 - val_loss: 0.1594 - val_rmse: 0.3992\n",
      "Epoch 139/500\n",
      "96/96 [==============================] - 0s 113us/sample - loss: 0.1317 - rmse: 0.3628 - val_loss: 0.1581 - val_rmse: 0.3976\n",
      "Epoch 140/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.1304 - rmse: 0.3611 - val_loss: 0.1568 - val_rmse: 0.3960\n",
      "Epoch 141/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.1292 - rmse: 0.3595 - val_loss: 0.1556 - val_rmse: 0.3945\n",
      "Epoch 142/500\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 0.1279 - rmse: 0.3576 - val_loss: 0.1545 - val_rmse: 0.3931\n",
      "Epoch 143/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.1268 - rmse: 0.3561 - val_loss: 0.1534 - val_rmse: 0.3916\n",
      "Epoch 144/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.1256 - rmse: 0.3544 - val_loss: 0.1522 - val_rmse: 0.3901\n",
      "Epoch 145/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.1243 - rmse: 0.3526 - val_loss: 0.1511 - val_rmse: 0.3887\n",
      "Epoch 146/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.1232 - rmse: 0.3510 - val_loss: 0.1499 - val_rmse: 0.3872\n",
      "Epoch 147/500\n",
      "96/96 [==============================] - 0s 108us/sample - loss: 0.1220 - rmse: 0.3492 - val_loss: 0.1488 - val_rmse: 0.3858\n",
      "Epoch 148/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.1208 - rmse: 0.3476 - val_loss: 0.1477 - val_rmse: 0.3844\n",
      "Epoch 149/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.1197 - rmse: 0.3460 - val_loss: 0.1467 - val_rmse: 0.3830\n",
      "Epoch 150/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.1186 - rmse: 0.3445 - val_loss: 0.1456 - val_rmse: 0.3815\n",
      "Epoch 151/500\n",
      "96/96 [==============================] - 0s 108us/sample - loss: 0.1175 - rmse: 0.3427 - val_loss: 0.1445 - val_rmse: 0.3801\n",
      "Epoch 152/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.1164 - rmse: 0.3411 - val_loss: 0.1435 - val_rmse: 0.3787\n",
      "Epoch 153/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1153 - rmse: 0.3395 - val_loss: 0.1424 - val_rmse: 0.3773\n",
      "Epoch 154/500\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 0.1141 - rmse: 0.3378 - val_loss: 0.1413 - val_rmse: 0.3759\n",
      "Epoch 155/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.1131 - rmse: 0.3362 - val_loss: 0.1402 - val_rmse: 0.3745\n",
      "Epoch 156/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.1119 - rmse: 0.3345 - val_loss: 0.1391 - val_rmse: 0.3730\n",
      "Epoch 157/500\n",
      "96/96 [==============================] - 0s 77us/sample - loss: 0.1108 - rmse: 0.3328 - val_loss: 0.1381 - val_rmse: 0.3717\n",
      "Epoch 158/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.1098 - rmse: 0.3313 - val_loss: 0.1371 - val_rmse: 0.3703\n",
      "Epoch 159/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.1086 - rmse: 0.3295 - val_loss: 0.1362 - val_rmse: 0.3690\n",
      "Epoch 160/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.1075 - rmse: 0.3279 - val_loss: 0.1352 - val_rmse: 0.3677\n",
      "Epoch 161/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.1065 - rmse: 0.3264 - val_loss: 0.1342 - val_rmse: 0.3664\n",
      "Epoch 162/500\n",
      "96/96 [==============================] - 0s 74us/sample - loss: 0.1055 - rmse: 0.3248 - val_loss: 0.1333 - val_rmse: 0.3651\n",
      "Epoch 163/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1045 - rmse: 0.3232 - val_loss: 0.1323 - val_rmse: 0.3637\n",
      "Epoch 164/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.1035 - rmse: 0.3217 - val_loss: 0.1314 - val_rmse: 0.3625\n",
      "Epoch 165/500\n",
      "96/96 [==============================] - 0s 75us/sample - loss: 0.1024 - rmse: 0.3200 - val_loss: 0.1306 - val_rmse: 0.3613\n",
      "Epoch 166/500\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 0.1015 - rmse: 0.3187 - val_loss: 0.1297 - val_rmse: 0.3601\n",
      "Epoch 167/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1006 - rmse: 0.3171 - val_loss: 0.1289 - val_rmse: 0.3590\n",
      "Epoch 168/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.0997 - rmse: 0.3157 - val_loss: 0.1281 - val_rmse: 0.3579\n",
      "Epoch 169/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.0987 - rmse: 0.3141 - val_loss: 0.1273 - val_rmse: 0.3568\n",
      "Epoch 170/500\n",
      "96/96 [==============================] - 0s 116us/sample - loss: 0.0979 - rmse: 0.3129 - val_loss: 0.1264 - val_rmse: 0.3556\n",
      "Epoch 171/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0970 - rmse: 0.3114 - val_loss: 0.1256 - val_rmse: 0.3544\n",
      "Epoch 172/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.0959 - rmse: 0.3097 - val_loss: 0.1248 - val_rmse: 0.3532\n",
      "Epoch 173/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.0951 - rmse: 0.3084 - val_loss: 0.1239 - val_rmse: 0.3519\n",
      "Epoch 174/500\n",
      "96/96 [==============================] - 0s 128us/sample - loss: 0.0941 - rmse: 0.3068 - val_loss: 0.1230 - val_rmse: 0.3508\n",
      "Epoch 175/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.0932 - rmse: 0.3053 - val_loss: 0.1222 - val_rmse: 0.3495\n",
      "Epoch 176/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.0923 - rmse: 0.3039 - val_loss: 0.1214 - val_rmse: 0.3484\n",
      "Epoch 177/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.0914 - rmse: 0.3023 - val_loss: 0.1206 - val_rmse: 0.3473\n",
      "Epoch 178/500\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 0.0905 - rmse: 0.3009 - val_loss: 0.1198 - val_rmse: 0.3461\n",
      "Epoch 179/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0896 - rmse: 0.2993 - val_loss: 0.1190 - val_rmse: 0.3450\n",
      "Epoch 180/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.0887 - rmse: 0.2978 - val_loss: 0.1183 - val_rmse: 0.3439\n",
      "Epoch 181/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.0879 - rmse: 0.2964 - val_loss: 0.1175 - val_rmse: 0.3428\n",
      "Epoch 182/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0869 - rmse: 0.2949 - val_loss: 0.1168 - val_rmse: 0.3418\n",
      "Epoch 183/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.0861 - rmse: 0.2935 - val_loss: 0.1160 - val_rmse: 0.3407\n",
      "Epoch 184/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.0854 - rmse: 0.2922 - val_loss: 0.1154 - val_rmse: 0.3397\n",
      "Epoch 185/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.0845 - rmse: 0.2906 - val_loss: 0.1147 - val_rmse: 0.3387\n",
      "Epoch 186/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.0837 - rmse: 0.2894 - val_loss: 0.1140 - val_rmse: 0.3377\n",
      "Epoch 187/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.0829 - rmse: 0.2879 - val_loss: 0.1134 - val_rmse: 0.3367\n",
      "Epoch 188/500\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 0.0822 - rmse: 0.2867 - val_loss: 0.1127 - val_rmse: 0.3357\n",
      "Epoch 189/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.0813 - rmse: 0.2852 - val_loss: 0.1120 - val_rmse: 0.3347\n",
      "Epoch 190/500\n",
      "96/96 [==============================] - 0s 113us/sample - loss: 0.0807 - rmse: 0.2840 - val_loss: 0.1114 - val_rmse: 0.3338\n",
      "Epoch 191/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.0799 - rmse: 0.2827 - val_loss: 0.1108 - val_rmse: 0.3328\n",
      "Epoch 192/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.0792 - rmse: 0.2814 - val_loss: 0.1102 - val_rmse: 0.3319\n",
      "Epoch 193/500\n",
      "96/96 [==============================] - 0s 145us/sample - loss: 0.0785 - rmse: 0.2802 - val_loss: 0.1096 - val_rmse: 0.3310\n",
      "Epoch 194/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.0778 - rmse: 0.2790 - val_loss: 0.1091 - val_rmse: 0.3303\n",
      "Epoch 195/500\n",
      "96/96 [==============================] - 0s 81us/sample - loss: 0.0772 - rmse: 0.2778 - val_loss: 0.1085 - val_rmse: 0.3294\n",
      "Epoch 196/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0765 - rmse: 0.2766 - val_loss: 0.1080 - val_rmse: 0.3286\n",
      "Epoch 197/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.0759 - rmse: 0.2755 - val_loss: 0.1074 - val_rmse: 0.3278\n",
      "Epoch 198/500\n",
      "96/96 [==============================] - 0s 76us/sample - loss: 0.0753 - rmse: 0.2745 - val_loss: 0.1069 - val_rmse: 0.3270\n",
      "Epoch 199/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0747 - rmse: 0.2733 - val_loss: 0.1064 - val_rmse: 0.3261\n",
      "Epoch 200/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.0742 - rmse: 0.2724 - val_loss: 0.1059 - val_rmse: 0.3254\n",
      "Epoch 201/500\n",
      "96/96 [==============================] - 0s 108us/sample - loss: 0.0735 - rmse: 0.2712 - val_loss: 0.1054 - val_rmse: 0.3247\n",
      "Epoch 202/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0730 - rmse: 0.2701 - val_loss: 0.1050 - val_rmse: 0.3240\n",
      "Epoch 203/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.0724 - rmse: 0.2691 - val_loss: 0.1045 - val_rmse: 0.3232\n",
      "Epoch 204/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.0719 - rmse: 0.2681 - val_loss: 0.1041 - val_rmse: 0.3226\n",
      "Epoch 205/500\n",
      "96/96 [==============================] - 0s 76us/sample - loss: 0.0713 - rmse: 0.2671 - val_loss: 0.1036 - val_rmse: 0.3219\n",
      "Epoch 206/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.0708 - rmse: 0.2662 - val_loss: 0.1031 - val_rmse: 0.3211\n",
      "Epoch 207/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.0703 - rmse: 0.2652 - val_loss: 0.1028 - val_rmse: 0.3206\n",
      "Epoch 208/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0699 - rmse: 0.2643 - val_loss: 0.1023 - val_rmse: 0.3199\n",
      "Epoch 209/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.0693 - rmse: 0.2633 - val_loss: 0.1020 - val_rmse: 0.3193\n",
      "Epoch 210/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.0688 - rmse: 0.2623 - val_loss: 0.1016 - val_rmse: 0.3188\n",
      "Epoch 211/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.0683 - rmse: 0.2614 - val_loss: 0.1013 - val_rmse: 0.3183\n",
      "Epoch 212/500\n",
      "96/96 [==============================] - 0s 77us/sample - loss: 0.0679 - rmse: 0.2605 - val_loss: 0.1009 - val_rmse: 0.3177\n",
      "Epoch 213/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.0674 - rmse: 0.2595 - val_loss: 0.1006 - val_rmse: 0.3172\n",
      "Epoch 214/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.0670 - rmse: 0.2588 - val_loss: 0.1002 - val_rmse: 0.3166\n",
      "Epoch 215/500\n",
      "96/96 [==============================] - 0s 81us/sample - loss: 0.0665 - rmse: 0.2579 - val_loss: 0.0999 - val_rmse: 0.3160\n",
      "Epoch 216/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.0661 - rmse: 0.2570 - val_loss: 0.0996 - val_rmse: 0.3156\n",
      "Epoch 217/500\n",
      "96/96 [==============================] - 0s 71us/sample - loss: 0.0657 - rmse: 0.2563 - val_loss: 0.0993 - val_rmse: 0.3152\n",
      "Epoch 218/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.0652 - rmse: 0.2554 - val_loss: 0.0991 - val_rmse: 0.3147\n",
      "Epoch 219/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.0648 - rmse: 0.2546 - val_loss: 0.0988 - val_rmse: 0.3143\n",
      "Epoch 220/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0644 - rmse: 0.2538 - val_loss: 0.0986 - val_rmse: 0.3139\n",
      "Epoch 221/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0640 - rmse: 0.2531 - val_loss: 0.0983 - val_rmse: 0.3135\n",
      "Epoch 222/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.0637 - rmse: 0.2524 - val_loss: 0.0980 - val_rmse: 0.3131\n",
      "Epoch 223/500\n",
      "96/96 [==============================] - 0s 116us/sample - loss: 0.0634 - rmse: 0.2517 - val_loss: 0.0977 - val_rmse: 0.3126\n",
      "Epoch 224/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.0630 - rmse: 0.2510 - val_loss: 0.0974 - val_rmse: 0.3121\n",
      "Epoch 225/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.0627 - rmse: 0.2503 - val_loss: 0.0972 - val_rmse: 0.3117\n",
      "Epoch 226/500\n",
      "96/96 [==============================] - 0s 73us/sample - loss: 0.0623 - rmse: 0.2497 - val_loss: 0.0971 - val_rmse: 0.3115\n",
      "Epoch 227/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.0620 - rmse: 0.2489 - val_loss: 0.0968 - val_rmse: 0.3111\n",
      "Epoch 228/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.0617 - rmse: 0.2483 - val_loss: 0.0966 - val_rmse: 0.3108\n",
      "Epoch 229/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.0614 - rmse: 0.2477 - val_loss: 0.0964 - val_rmse: 0.3105\n",
      "Epoch 230/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.0610 - rmse: 0.2471 - val_loss: 0.0962 - val_rmse: 0.3102\n",
      "Epoch 231/500\n",
      "96/96 [==============================] - 0s 81us/sample - loss: 0.0607 - rmse: 0.2464 - val_loss: 0.0961 - val_rmse: 0.3099\n",
      "Epoch 232/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.0604 - rmse: 0.2458 - val_loss: 0.0959 - val_rmse: 0.3097\n",
      "Epoch 233/500\n",
      "96/96 [==============================] - 0s 74us/sample - loss: 0.0602 - rmse: 0.2453 - val_loss: 0.0957 - val_rmse: 0.3093\n",
      "Epoch 234/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.0598 - rmse: 0.2446 - val_loss: 0.0955 - val_rmse: 0.3090\n",
      "Epoch 235/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.0595 - rmse: 0.2440 - val_loss: 0.0953 - val_rmse: 0.3088\n",
      "Epoch 236/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0593 - rmse: 0.2435 - val_loss: 0.0952 - val_rmse: 0.3086\n",
      "Epoch 237/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.0590 - rmse: 0.2429 - val_loss: 0.0951 - val_rmse: 0.3083\n",
      "Epoch 238/500\n",
      "96/96 [==============================] - 0s 75us/sample - loss: 0.0587 - rmse: 0.2423 - val_loss: 0.0949 - val_rmse: 0.3081\n",
      "Epoch 239/500\n",
      "96/96 [==============================] - 0s 73us/sample - loss: 0.0585 - rmse: 0.2418 - val_loss: 0.0948 - val_rmse: 0.3079\n",
      "Epoch 240/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.0582 - rmse: 0.2413 - val_loss: 0.0946 - val_rmse: 0.3076\n",
      "Epoch 241/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.0579 - rmse: 0.2407 - val_loss: 0.0944 - val_rmse: 0.3073\n",
      "Epoch 242/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0576 - rmse: 0.2401 - val_loss: 0.0943 - val_rmse: 0.3071\n",
      "Epoch 243/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.0574 - rmse: 0.2396 - val_loss: 0.0942 - val_rmse: 0.3069\n",
      "Epoch 244/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0572 - rmse: 0.2391 - val_loss: 0.0940 - val_rmse: 0.3067\n",
      "Epoch 245/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.0569 - rmse: 0.2385 - val_loss: 0.0940 - val_rmse: 0.3065\n",
      "Epoch 246/500\n",
      "96/96 [==============================] - 0s 81us/sample - loss: 0.0567 - rmse: 0.2380 - val_loss: 0.0939 - val_rmse: 0.3064\n",
      "Epoch 247/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.0564 - rmse: 0.2375 - val_loss: 0.0938 - val_rmse: 0.3062\n",
      "Epoch 248/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.0562 - rmse: 0.2370 - val_loss: 0.0936 - val_rmse: 0.3060\n",
      "Epoch 249/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.0559 - rmse: 0.2365 - val_loss: 0.0935 - val_rmse: 0.3059\n",
      "Epoch 250/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.0557 - rmse: 0.2360 - val_loss: 0.0935 - val_rmse: 0.3058\n",
      "Epoch 251/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.0555 - rmse: 0.2355 - val_loss: 0.0934 - val_rmse: 0.3056\n",
      "Epoch 252/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.0553 - rmse: 0.2351 - val_loss: 0.0933 - val_rmse: 0.3054\n",
      "Epoch 253/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.0550 - rmse: 0.2346 - val_loss: 0.0933 - val_rmse: 0.3054\n",
      "Epoch 254/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.0547 - rmse: 0.2340 - val_loss: 0.0932 - val_rmse: 0.3053\n",
      "Epoch 255/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0545 - rmse: 0.2335 - val_loss: 0.0931 - val_rmse: 0.3052\n",
      "Epoch 256/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.0543 - rmse: 0.2330 - val_loss: 0.0931 - val_rmse: 0.3051\n",
      "Epoch 257/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.0541 - rmse: 0.2325 - val_loss: 0.0930 - val_rmse: 0.3050\n",
      "Epoch 258/500\n",
      "96/96 [==============================] - 0s 77us/sample - loss: 0.0539 - rmse: 0.2321 - val_loss: 0.0930 - val_rmse: 0.3050\n",
      "Epoch 259/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.0536 - rmse: 0.2316 - val_loss: 0.0930 - val_rmse: 0.3049\n",
      "Epoch 260/500\n",
      "96/96 [==============================] - 0s 73us/sample - loss: 0.0534 - rmse: 0.2310 - val_loss: 0.0929 - val_rmse: 0.3048\n",
      "Epoch 261/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.0532 - rmse: 0.2306 - val_loss: 0.0929 - val_rmse: 0.3048\n",
      "Epoch 262/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.0530 - rmse: 0.2302 - val_loss: 0.0929 - val_rmse: 0.3048\n",
      "Epoch 263/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0528 - rmse: 0.2298 - val_loss: 0.0928 - val_rmse: 0.3046\n",
      "Epoch 264/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.0526 - rmse: 0.2293 - val_loss: 0.0928 - val_rmse: 0.3046\n",
      "Epoch 265/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.0524 - rmse: 0.2289 - val_loss: 0.0927 - val_rmse: 0.3045\n",
      "Epoch 266/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0523 - rmse: 0.2286 - val_loss: 0.0928 - val_rmse: 0.3047\n",
      "Epoch 267/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.0521 - rmse: 0.2282 - val_loss: 0.0928 - val_rmse: 0.3047\n",
      "Epoch 268/500\n",
      "96/96 [==============================] - 0s 81us/sample - loss: 0.0519 - rmse: 0.2278 - val_loss: 0.0928 - val_rmse: 0.3046\n",
      "Epoch 269/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.0517 - rmse: 0.2275 - val_loss: 0.0927 - val_rmse: 0.3045\n",
      "Epoch 270/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.0516 - rmse: 0.2271 - val_loss: 0.0927 - val_rmse: 0.3045\n",
      "Epoch 271/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.0514 - rmse: 0.2268 - val_loss: 0.0928 - val_rmse: 0.3046\n",
      "Epoch 272/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.0513 - rmse: 0.2264 - val_loss: 0.0928 - val_rmse: 0.3046\n",
      "Epoch 273/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0511 - rmse: 0.2261 - val_loss: 0.0928 - val_rmse: 0.3046\n",
      "Epoch 274/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.0510 - rmse: 0.2257 - val_loss: 0.0928 - val_rmse: 0.3046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error in Senate Polarity Training Model by Epoch Number')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VPXZ///XO3sCYU9YQ1gFBQEhILivFami1lrFqtWqdNHa1trW3r3v/tRvl9tuWm9trVrcq0VbFVurVVxRUCKboiLIGtYQCEtYQpLr98c5wUnIMoEMk0mu5+Mxj5mzzDnXmTNzrjmfzzmfj8wM55xzrlpSvANwzjnXsnhicM45V4MnBuecczV4YnDOOVeDJwbnnHM1eGJwzjlXgyeGKEg6UdKSeMeRKCSZpEEH+d6+knZKSm7uuA6VpAGSdjb3vC2FpCJJp0Qx3yBJ9V7nHu1yWiJJ10h6Pd5x1EfSY5JuifV6WnxikLRS0u7wYFH9uPtwxmBmb5nZkIN5r6Q0Sb8Lfyw7Ja2QdEdzx1jHeq+UNOsQ318Zxrxd0gJJ5zRnjHUxs9Vm1t7MKsM4Xpd0TVOXE5Fgqh8mqSxi+MSDiG25mbVv7nmbKjw4mKRJtcbfHY6/LBbrPdwknSGpqtZ+3ClpbLxjqxb+rtdLyooY901Jr8QzrkPV4hND6NzwYFH9uL6umSSlRDOuIU2dPwo/AQqAcUA2cCowv5nXESuzw4NbJ+AvwHRJXWK1sub87CMSTPuIA/TIiHFv1bH+FneW0oBPga9VD0hKBS4ElsctothYXeu3397M5sY7qFrSgDqPSS1ZQ9/3REkMdQr/1b4t6Q5JW4Bb6hmXJOm/Ja2StEnSI5I6hsvoF/7LulrSauDVOtZziqSiiOGVkm6StEjSNkl/k5RRT5hjgWfMbJ0FVprZIxHL6iXp75KKw7OJGyKm3SJpehjvDkmLJRVETL9Z0mfhtI8kXRCOPxK4F5gQ/sMqDcenS/qtpNWSNkq6V1JmY5+zmVUB04BMYEC4rGslLZO0RdIMSb3q2UdflDQ/POtYE3kaXNdnHzEuRdIvgBOBu6vPFCXdI+l3tdbxvKTvNbYddcT2WLi8FyWVASdKmhyeHe0IP6f/iZi/RhGKpFmSbpX0Tjj/i9WJsynzhtOvCte3WdJ/qfHimGeBU6q/x8AXgUKgOGKZSZJ+FvG9f0hSh4jpV4bTNku6udZnkxTG8Vk4/UlJnZvw8R4r6WNJWyX9RVJ6uNxPJJ0dsZ70cJ7hTVh29XtnSfqFpMLwd/hMZIySzg9/M6WSXpU0JGJavqRnw9/dZkl/qLlo3RG+b7mkLzQSyq+BH0V+thELOqDYLYz7yvD1NZLekHRXuL5lko4NfxNrwt9p7TPAHEkzw+/Ra5LyIpZ9lKRXwt/lJ5IujJh2wPe93i0ysxb9AFYCZ9Qz7UqgAvgOkEJw4Kpr3NeBZQQHtfbAP4BHw2X0Awx4BGgHZNaxnlOAoloxvQf0AroAHwPfrCfG/wZWA98GjgYUMS0JeB/4GcG/jgEE//jOCqffAuwBJgHJwK+AORHvvyiMIQm4GCgDekZ8NrNqxXInMCOMORt4HvhVA5/trPB1CvBdYAfQETgN2AyMBtKB/wPejHivAYMiPrujwxhHABuB8+v77CPGpYTzvA5cE7HsccA6ICkc7gbsAro38j3aH1PEuMeArcCEML70cNuGh8Mjw+08J5x/EGAR758FLAUGA1nAW8DPD2Leo8PP9rgwhjsIvsOn1LMtj4XfjWnAteG4f4TfhznAZeG4qQRnFv3D/f0c8GDEOncCx4frvCtyncBNwNtAbyCD4Izx0bq2rY74ioBFQJ9w/8wBbgmn/RfweMS8FwLz61nOGcDKBtYzC1gDHBV+f54FHgqnHRlu32lAarjeT8PXKcCHwG/5/Ht3fPi+a4B9BMeMZILjyJpGtvUUgt9V9TZ+E3ilvs8qjPvKiPVVAJeH6/tfYFW4P9IJfvvbgKyIfb8tYr/dA7weTssG1gJXhNs4BigBhtT3fa93uw7mYH04HwQH4Z1AacSj+sdwJcGpZu0DWu1xM4FvRwwPCXd+Cp8fiAY0EMMpHJgYLosY/jVwbz3vTQauI/iR7SU4qH0tnHZsHbH+hM9/vLdUf8HC4aOA3Q3EuQA4L+JzmBUxTQSJY2DEuAnAinqWdWX4hS0lODjOIUzQBAeJX0fM2z78PPuFwwcchCPmvRO4I3x9wGdPI4khHPcxcGb4+nrghSi+R/UlhmmNvO9u4Dfh67oO9jdHDN8A/PMg5r2N8KAbDrcjusRwCkGC6QJsIDhQRCaGN4CpEe8bFn4Hk8J1PlZrH1byeWJYCpwcMT0v4r3RJIbIZD4ZWBKxnO1A+3D4WeDGepZzBlBFzd9+KeEBLfxMfx4x/wiCP1ICbgX+GjEtKfyMTiD4p7wBSK5jndcAn0QMdwi/O90a2NZTCP5ElAJdaXpi+Dhi2jHh+rpGjNsGDI/Y95H7rWP4GfUEvgq8VmtdfwF+Gu33vfrR3OXpsXK+mdVXmbMminG9CLJwtVUESaF7I8tpyIaI17vCdRzAgkrUe4B7FBTbfB2YJuk9IB/opbCoJ5RM8GOvbz0ZklLMrELSFcCNBAdTCH7c3eqJN4fgn+r7kqrHKVxffeaY2Ql1jO8FzIvYxp2SSgj+Xa6MnFHSsQT/goYTnBWlA0/VWl5TP/uHgcuAl8PnPzQ8e4NqrFvSBIIzs2F8Hu8TDby/9v5pqMK5vnl7RcZhZmWStjYaeXDg70Pwb/g5M9sbsW+rl1v7e59G8F2ovc6dCopeq/UFnpdUFTHOgNwo4oKan+uqcH2Y2Zrwu3+BpH8BXyA4kNZntZn1a8J60gkSZY1tN7MqBcXBvQm+8yvD32Zdau8nCPbV5vqCMLOFkl4CfgSsaCDeumyMeL0bqDSzklrjIr9Xkfttm6RtBNubDxxf63iSAjxU13sbkiiJoSEWxbh1BB9atb4E/8g2Evyw6ltOszKz3QQJ4laCf/9rCP6xD27qsiTlA/cDpxNUEldKWkBwsIcDt2czwRdsmJmtPdhtCNX4PCW1I/inVNdy/0rwr/tsM9sj6U4OTF4NffZ1TXsM+FDSSIIig2ebEHtjy3+SoIhhYhjv3TR8sG8O6znw82y0PN/MTNLjBImhrvLiur735QT1EOsJipiq19me4IBarQi41Mzerb1QRXcpcl7E675hLNWqE3t7giLIyANxU9Vez15gS7i+/b8rSUkEv/W1BL+RfEnJDSSHg/EzYC7w+4hxZeH6s8ysOsn0OMT1RNYpdCQ4a1hHcDyZaWZn1/dGojzOJXTlcxM8AXxfUv/wB/BL4G9mVhHrFUv6noLK60wFFapfIygLnE9QT7Fd0o/D6cmShiu6y/HaEezk4nA9VxH8K6+2EegjKQ32VyDfD9whKTd8T29JZx3EZv0VuErSqLBS8ZfAu2a2so55s4Et4UF2HHBpE9e1kbDCu5qZFRH8AB8F/h4m3OYSGe944JJmXHZ9ngLOlzQ+3F+3NeG9dxAUq71dx7QngBsVVOhnA78Angi/C08B50maEO7Dn1PzoHEv8EtJfQEk5Uqa3IS4rg+/X10Jikf/FjHtHwTFqNcT1C8diiskDQ2T6a3AdAvKTaYDk8PfXirwQ4J6nHeB2QRl77+UlBX+9o4/xDgwsyXA3wnqJaptCB+Xhb/vqdRM1gfj3Fr7bZaZrSeo5xgm6VJJqeFjXGSle7QSJTE8r5rXMT/TxPdPIziIvElwmreHmjsvlnYDvyP4cmwmqG+40ILr3CuBc4FRYVybgQcI/gE0yMw+Cpc7m+DgeTRBPUa1V4HFwAZJ1afAPyaohJ8jaTvwCkF9S5OY2Uzgfwh+BOuBgdR/AP02cJukHQT/qKY3cXV/AL6s4MqVuyLGP0ywzY82cXmN+RbwqzDe/6Lp8TaZmS0Cvk9wsF5HcNAqIfj329h7S8L9UZf7CQ7IbxFc1LCD4CKC6nV+l2D71vL5Aaza74EXgZnhZ/EOwRV20XqC4Pv1GbCE4M9DdcxlBGd5fWn8bK/2/Sg7JZ0fMf1RgjPI9QRFRN8L17GY4HLePxH8eZoITDazfeEfwnMIzjbXEFwc8uUmbFtDbiXiDDNMUtcSfJc2E9Q5HHAW1kSPESSEzQT1KpeH69oGnEVwNraeYH/+iqB4rUkUVko4l1AknUTwA+kX/gNuNRRc9lgK5JtZU+tfEoKk24C+ZnblISxjFvCAmT3UXHG5QKKcMTi3X1g08F2Cg0KrSAoK7p/ICos6fwfMa8VJoStwFXBfvGNxdfPE4BKKgpv3Sgkuz7szzuE0pwsIipGKCK4ymxLXaGJE0rcIim6eM7N34h2Pq5sXJTnnnKvBzxicc87VkHD3MXTr1s369esX7zCccy6hvP/++5vNLCeaeRMuMfTr14/CwsJ4h+GccwlF0qrG5wp4UZJzzrkaPDE455yrwRODc865GjwxOOecq8ETg3POuRo8MTjnnKvBE4Nzzrka2kximLd6K7e/+Em8w3DOuRavzSSGD9du40+vf8bSjTviHYpzzrVobSYxnDUs6E3v3x8eSi+CzjnX+rWZxNC9QwZj8jvzoicG55xrUMwSg6RpkjZJ+rCe6ZJ0l6RlkhZJGh2rWKpNHNaDj9ZvZ3XJrsZnds65NiqWZwwPEfSzWp+zgcHhYypB36wxNXF4UJz04uL1sV6Vc84lrJglBjN7E9jSwCznAY9YYA7QSVLPWMUDkNcli+G9O3g9g3PONSCedQy9gcg+bYvCcQeQNFVSoaTC4uLiQ1rpxGE9mL+6lPXbdh/ScpxzrrWKZ2JQHePq7GfUzO4zswIzK8jJiaqfiXpNHB6clHgltHPO1S2eiaEIyIsY7kPQGXpMDcptz5Du2bzwgdczOOdcXeKZGGYAV4RXJ40HtpnZYTlaf3FETwpXbWXDtj2HY3XOOZdQYnm56hPAbGCIpCJJV0v6pqRvhrO8ACwHlgH3A9+OVSy1TTq6J2bw7w/9rME552qLWZ/PZjalkekGXBer9TcksjjpquP7xyME55xrsdrMnc+1TTrai5Occ64ubTYxfHFEDy9Ocs65OrTZxDAoN9uvTnLOuTq02cQAXpzknHN1adOJwYuTnHPuQG06MXhxknPOHahNJwbw4iTnnKutzScGL05yzrma2nxi8OIk55yrqc0nBvDiJOeci+SJAS9Ocs65SJ4Y+Lw46V+LPDE455wnhtB5x/SicNVWlhfvjHcozjkXV54YQl8e3YfkJPG3wjWNz+ycc62YJ4ZQbocMThuay9/fL2JfZVW8w3HOubjxxBDhkrF5bN5ZzsyPN8Y7FOeci5uYJgZJEyUtkbRM0s11TM+XNFPSIkmvS+oTy3gac/IROfTokMGTc704yTnXdsWya89k4B7gbOAoYIqko2rN9lvgETMbAdwG/CpW8UQjJTmJiwr68Manxawr3R3PUJxzLm5iecYwDlhmZsvNrBx4Ejiv1jxHATPD16/VMf2w+0pBHmbwVGFRvENxzrm4iGVi6A1ElskUheMiLQQuDF9fAGRL6lp7QZKmSiqUVFhcXByTYKvldcnixMHdmF64hsoqi+m6nHOuJYplYlAd42ofaW8CTpY0HzgZWAtUHPAms/vMrMDMCnJycpo/0louHpvH2tLdzFq2Oebrcs65liaWiaEIyIsY7gOsi5zBzNaZ2ZfM7Bjgp+G4bTGMKSpnHtWdzlmp/G3u6niH4pxzh10sE8NcYLCk/pLSgEuAGZEzSOomqTqGnwDTYhhP1NJTkvnS6D68/NFGNu/cG+9wnHPusIpZYjCzCuB64CXgY2C6mS2WdJukyeFspwBLJH0KdAd+Eat4muqSsXnsqzT+Mc8roZ1zbYvMEquCtaCgwAoLCw/Lui780zts3VXOzBtPRqqrysQ55xKDpPfNrCCaef3O5wZcPDaP5cVlFK7aGu9QnHPusPHE0IBzRvSkfXoKT77nd0I759oOTwwNyEpLYfKoXvzrg3Vs270v3uE459xh4YmhEZeMzWPPvipmLFgb71Ccc+6waDAxSEqS9JXDFUxLdHTvjgzv3YHH5qwm0SrqnXPuYDSYGMysiuCS0zZLEpePz2fJxh28t2JLvMNxzrmYi6Yo6WVJN0nKk9Sl+hHzyFqQySN70zEzlUfmrIp3KM45F3MpUczz9fD5uohxBgxo/nBapsy0ZC4a04eH3lnJpu17yO2QEe+QnHMuZho9YzCz/nU82kxSqHbZ+Hwqqoy/vuftJznnWrdGE4OkVEk3SHo6fFwvKfVwBNeS9OvWjlOG5PDYnNXs2VcZ73Cccy5moqlj+BMwBvhj+BgTjmtzpp44gM079/KPeX7pqnOu9YqmjmGsmY2MGH5V0sJYBdSSTRjYlRF9OnLfm59x8dg8kpO8/STnXOsTzRlDpaSB1QOSBgBtsixFEt84aSArS3bxn8Ub4h2Oc87FRDRnDD8EXpO0nKBXtnzgqphG1YJNHN6Dfl2zuPeNz5g4vIe3uuqca3UavfMZ2A0MBm4IH0PM7LXDEFuLlJwkrj1pAAuLtjFnud/w5pxrfaK58/l3ZrbXzBaZ2UIza/Ndml04ug/d2qdx7xufxTsU55xrdtHUMfxH0oU6iDITSRMlLZG0TNLNdUzvK+k1SfMlLZI0qanriIeM1GSuOr4/b3xazMfrt8c7HOeca1bRJIYbgaeAvZK2S9ohqdGjoaRk4B7gbOAoYIqko2rN9t8EXX4eQ9An9B+bFH0cffXYvmSmJvPAWyviHYpzzjWrxuoYBAwzsyQzSzOzDmaWbWYdolj2OGCZmS03s3LgSeC8WvMYUL2sjsC6JsYfN52y0rh4bB4zFq5l/bbd8Q7HOeeaTWN1DAY8c5DL7g1Edn1WFI6LdAtwmaQi4AXgO3UtSNJUSYWSCouLiw8ynOZ39Qn9MYM/vuZ1Dc651iOaoqQ5ksYexLLrqpOo3aHBFOAhM+sDTAIeDa+Eqvkms/vMrMDMCnJycg4ilNjI65LFV8bm8eTc1RRt3RXvcJxzrllEkxhOBWZL+iysIP5A0qIo3lcE5EUM9+HAoqKrgekAZjYbyAC6RbHsFuP6UwchxP/NXBbvUJxzrllEkxjOBgYCpwHnAueEz42ZCwyW1F9SGkHl8oxa86wGTgeQdCRBYmg5ZUVR6NUpk0uP7cvT84pYubks3uE459whqzcxSDoNwMxWAUlmtqr6QdCQXoPMrIKg97eXgI8Jrj5aLOk2SZPD2X4AXBu2vfQEcKUlYP+Z3z5lIKnJ4g8zl8Y7FOecO2Sq7zgsaZ6Zja79uq7hw6mgoMAKCwvjseoG/fKFj7n/reW8/P2TGJSbHe9wnHOuBknvm1lBNPM2VJSkel7XNdzmfeOkAWSlJnPHK37W4JxLbA0lBqvndV3DbV7X9ulcdXx//rVoPR+t87uhnXOJq6HEMEDSDEnPR7yuHu5/mOJLKNeeOIDsjBR+//KSeIfinHMHraFmtyPvUv5trWm1hx3QMSuVb5w0gN/+51PeX7WFMfld4h2Sc841Wb2JwczeOJyBtBZfP6E/D72zitv/vYS/fWO899fgnEs40dzH4JogKy2F754+iPdWbuH1JQl1S4ZzzgGeGGLiknF9ye+axe0vfkJVldfTO+cSS9SJQVK7WAbSmqQmJ3HjmUfwyYYdzFiYMA3GOuccEEVikHScpI8I7l5G0khJCdNvQrycO6IXR/XswO9eXkJ5RVW8w3HOuahFc8ZwB3AWUAJgZguBk2IZVGuQlCR+NHEIa7bs5sm5q+MdjnPORS2qoiQzW1NrVGUMYml1Tj4ih2P7d+GumUsp21sR73Cccy4q0SSGNZKOA0xSmqSbCIuVXMMk8eOzh7J5ZznTZnkXoM65xBBNYvgmcB1B72tFwKhw2EVhdN/OfOGo7vz5zeVsKSuPdzjOOdeoxvp8TgYuN7Ovmll3M8s1s8vMrOQwxdcq/GjiEHbvq/SmMpxzCaGxPp8rqdk0hjsIg3KzuXx8Pn99d7U3sOeca/GiKUp6W9Ldkk6UNLr6Ec3CJU2UtETSMkk31zH9DkkLwsenkkqbvAUJ4vtnHEHHzFRufX4xCdgXkXOuDWmoEb1qx4XPt0WMM4KuPusVFkPdA5xJUDcxV9IMM/to/0LMvh8x/3eAY6KMO+F0zErlB18Ywn8/+yH/XLSec0f2indIzjlXp0YTg5mdepDLHgcsM7PlAJKeJCiW+qie+acA/99BrishTBnXlyfnruaWGYs5flA3urRLi3dIzjl3gKjuY5D0RUk/kvSz6kcUb+sNRN7/UBSOq2v5+QR9PLwaTTyJKjlJ/PaikWzfs49f/Muv+HXOtUzRNIlxL3Ax8B2CLj0vAvKjWHZd7U3XV7h+CfB0WNldVwxTJRVKKiwuTuwWS4f26MDXT+jPP+YX8eHabfEOxznnDhDNGcNxZnYFsNXMbgUmAHlRvK+o1nx9gPpalLsEeKK+BZnZfWZWYGYFOTk5Uay6Zfv2KYPoFFZEe+urzrmWJprEsDt83iWpF7CP6Lr2nAsMltRfUhrBwX9G7ZkkDQE6A7OjCznxdcxM5b8mHcnclVt56J2V8Q7HOedqiCYx/FNSJ+A3wDxgJfBkY28yswrgeuAlgiY0ppvZYkm3SZocMesU4ElrY9dwfnlMH04bmsuvX/qE5cU74x2Oc87tp6YcjyWlAxlmFrfC8YKCAissLIzX6pvVxu17OPP3bzC4ezbTvzGB5CTvBtQ5FxuS3jezgmjmbfRyVUlX1DEOM3vkYIJzn+veIYNbzxvG9/+2kGmzVnDtSQPiHZJzzkV1g9vYiNcZwOkERUqeGJrB+aN688IHG/jNf5Zw6tBcBuW2j3dIzrk2rtE6BjP7TsTjWoK7k/3OrGYiiV9cMJystGR+8NRCKiq9tzfnXHxF3edzhF3A4OYOpC3Lzc7gtvOGs3BNKfe/5f02OOfiK5o6huf5/Ma0JOAoYHosg2qLzh3Rkxc/XM8dL3/KKUNyOLJnh3iH5Jxro6KpY/htxOsKYJWZFcUonjZLEv/vvOEUrnyLbz8+jxnXH092Rmq8w3LOtUHR1DG8EfF425NC7HRtn87/TTmG1Vt28eO/L/LmuZ1zcRFNW0k7JG2v47FDkvc608yOHdCVH501hBc+2MCDb6+MdzjOuTYomqKkO4ANwKMEDeN9Fcg2s1/HMrC2bOpJAyhctZVfvvAxI/M6MSa/c7xDcs61IdFclXSWmf3RzHaY2XYz+xNwYawDa8ukoHnuXp0yuf6v8yjZuTfeITnn2pBoEkOlpK9KSpaUJOmrQJ3NY7vm0zEzlT9+dTQlZeV8728LqPRWWJ1zh0k0ieFS4CvARmATQX8Ml8YyKBcY3rsjt00exltLN3PXzKXxDsc510ZE07XnSoIuOV0cXDw2j7krt3LXq0sZnd+Zk49I/P4onHMtW71nDJKulTQ4fC1J0yRtk7RI0ujDF2LbJomfnz+cId2z+d6T81lXurvxNznn3CFoqCjpuwR9L0DQZ8JIYABwI/CH2IblImWmJfPHr45mX6Vx3V/nUV7h7Sk552KnocRQYWb7wtfnAI+YWYmZvQK0i31oLtKAnPbcfuEI5q8u5Vf//jje4TjnWrGGEkOVpJ6SqpvafiViWmY0C5c0UdISScsk3VzPPF+R9JGkxZL+Gn3obc8XR/TkquP78eDbK/nnovq6z3bOuUPTUOXzz4BCIBmYYWaLASSdDCxvbMGSkoF7gDOBImCupBlm9lHEPIOBnwDHm9lWSbkHvSVtxE/OPpIFa0r58dOLGNojm0G52fEOyTnXytR7xmBm/wTygSPDfhiqFQIXR7HsccAyM1tuZuUE/UTXvrrpWuAeM9sarnNTU4Jvi9JSkrjn0tFkpiVz9cOFbC0rj3dIzrlWpsH7GMysovqgHTGuzMyi6b2+N7AmYrgoHBfpCOAISW9LmiNpYl0LkjRVUqGkwuLi4ihW3br16pTJny8fw/pte/jGY+97ZbRzrlkdTEc90aqrZ/vat++mEHT6cwrBlU8PSOp0wJvM7jOzAjMryMnx6/gBxuR34dcXjuC9FVv472c/8JZYnXPNJppG9A5WEZAXMdwHqF1jWgTMCa9+WiFpCUGimBvDuFqN84/pzfLindz16jL6d2vPt04ZGO+QnHOtQFSJQVJvgvqG/fOb2ZuNvG0uMFhSf2AtcAkHNqXxLMGZwkOSuhEULTVase0+970zjmBFyS5uf/ETcrLT+fKYPvEOyTmX4KLp2vN2gsrmj/i88TwDGkwMZlYh6XrgJYIrm6aZ2WJJtwGFZjYjnPYFSdXL/qGZlRz01rRBSUnitxeNYEvZXn7890V0zkrl9CO7xzss51wCU2Nl02HxzggzaxFtPxcUFFhhYWG8w2hxdu6tYMp9c1i6aQePX3MsY/K7xDsk51wLIul9MyuIZt5oKp+XA975cAvXPj2FB68aS8+OmVz14FyWbNgR75CccwkqmsSwC1gg6c+S7qp+xDow13Td2qfzyNfHkZGazBXT3qVo6654h+ScS0DRJIYZwP8D3gHej3i4FiivSxYPf30cu8orueIv77Fp+554h+ScSzCN1jG0NF7HEJ33Vmzhygffo0u7NB75+jgG5LSPd0jOuThq1joGSYMlPR02dLe8+nHoYbpYGte/C09cO57d5ZV8+d7ZLFhTGu+QnHMJIpqipAeBPwEVwKnAI8CjsQzKNY+ReZ14+lvH0S49mSn3zeG1Jd4UlXOucdEkhkwzm0lQ7LTKzG4BTottWK659O/Wjr9/6zgG5LTjmocLefr9oniH5Jxr4aJJDHskJQFLJV0v6QLAm8dOILnZGTw5dTzjB3ThpqcW8qfXP/O2lZxz9YomMXwPyAJuAMYAlwFfi2VQrvllZ6Ty4JXjmDyyF7e/+Am3Pv8RVVWeHJxzB2q0SQwzmwsgyczsqtiH5GIlLSWJOy8eRU52On+ZtYLiHXv57UUjyUxLjndozrkWJJqrkiaEbRl9HA6PlPTHmEfmYiIpSfzPOUfx00lH8sKH6/nyve+wZovfCOec+1w0RUl3AmcBJQBmthA4KZZBudi79qQBTPvaWFZv2cXku2fx6icb4x2Sc66FiKqjHjNbU2tHFgpBAAAXcUlEQVRUZZ0zuoRy6tBcZlx/At07ZPD1hwq59fnF7K3wXetcWxdNYlgj6TjAJKVJuomwWMklvv7d2vHsdcdz5XH9ePDtlVxwzzt8VhxNz63OudYqmsTwTeA6gv6ai4BR4bBrJTJSk7ll8jAeuKKA9dt2c85ds5heuMYvaXWujfK2klwNG7bt4ft/W8Ds5SVMOroHt5w7jNwOGfEOyzl3iJrSVlK9l6s21rS2md0QRSATgT8Q9OD2gJn9b63pVwK/Iej6E+BuM3ugseW62OnRMYPHrjmWP7/5GXe+spS3Pt3MTWcN4bLx+SQnKd7hOecOg3rPGCSVAx8C04F1QI2jgpk93OCCpWTgU+BMgiKoucAUM/soYp4rgQIzuz7agP2M4fBZsbmMnz33IW8t3cyY/M7cfuEIBuV6K63OJaLmal21J3AfwaWqlxP04jbDzB5uLCmExgHLzGy5mZUDTwLnRROUaxn6d2vHI18fx+8uGsmyTTuZdNdb/P4/S9hd7lcuOdea1ZsYzKzEzO41s1OBK4FOwGJJl0e57N5A5GWuReG42i6UtChs2juvrgVJmiqpUFJhcXFxlKt3zUESF47pw8s3nsTEYT2469VlnP671/nXovVeOe1cKxXNnc+jCdpLugz4N9H33lZXgXTtI8nzQD8zGwG8AtR5JmJm95lZgZkV5OTkRLl615xyszO4a8oxTP/GBDpmpXHdX+dxyX1z+Hj99niH5pxrZvUmBkm3SnofuBF4g6Au4OrIOoJGFAGRZwB9COoq9gvPSvaGg/cTNNLnWrBx/bvwz++cwM/PH86SjTv44l1v8dNnPqBk597G3+ycSwgNVT5XAcuB3eGo6hkFWPgvv/4FSykElc+nE1x1NBe41MwWR8zT08zWh68vAH5sZuMbWq5XPrccpbvKufOVpTw6ZxVZqcl885SBXHV8P7LSGm2b0Tl3mDWl8rmhxJDf0BvNbFUUgUwiaGspGZhmZr+QdBtQaGYzJP0KmEzQO9wW4Ftm9klDy/TE0PIs3biD219cwisfbyQnO50bTh/MJWPzSE2OqsUV59xh0CyJoaXyxNByFa7cwu0vfsLclVvJ75rFD74whHOO7kmS3//gXNw11+WqzjVJQb8uTP/GBB68ciyZqcnc8MR8zr17Fm98WuxXMDmXQDwxuGYliVOH5vLCDSdyx8Uj2bZ7H1+b9h5T7p/D/NVb4x2ecy4K0Vyu+t1oxjkXKSlJXHBMH179wSncOnkYyzbt5II/vsPVD83lw7Xb4h2ec64BjdYxSJpnZqNrjZtvZsfENLJ6eB1DYirbW8FD76zkz298xvY9FZw9vAffP/MIjuieHe/QnGsTmuuqpCnApcAJwFsRk7KBSjM741ADPRieGBLbtt37+MusFUybtYKy8gomj+zFDacPZmCOt8HkXCw15+Wq/YFfATdHTNoBLDKzikMN9GB4YmgdtpaV8+c3l/PwOyvZU1HJpOE9+fapAxnWq2O8Q3OuVWr2y1UldQfGhoPvmdmmQ4jvkHhiaF0279zLtFkreGT2KnbureD0oblcd9ogRvftHO/QnGtVmjUxSLoI+C3wOsFdzycCPzSzpw8xzoPiiaF12rZ7H4+8s5Jpb69g6659HD+oK9efOpjxA7og+X0Qzh2q5k4MC4Ezq88SJOUAr5jZyEOO9CB4YmjdyvZW8Nd3V3PfW8sp3rGXMfmdmXrSAM44srt3FOTcIWjuxPCBmR0dMZwELIwcdzh5Ymgb9uyr5KnCNdz7xnLWlu4mv2sWVx3Xj4sK8miX7m0xOddUzZ0YfgOMAJ4IR11MUPn840OK8iB5YmhbKiqreGnxRh6YtZz5q0vpkJHCZePzufK4ft4XtXNNEIvK5y8RXLYq4E0ze+bQQjx4nhjarvdXbeWBt5bz0uINJCeJ80b15poT+zO0R4d4h+Zci9eUxBDtOfnbwD6CprffO9jAnDsUY/I7MyZ/DKtKypg2awXTC4t4+v0iThzcjatP6M/JR+R4RbVzzSCaoqSvAL/Br0pyLUzprnIef3c1j8xeycbtexmU256rT+jPBcf0JiM1Od7hOdei+FVJrk0pr6jiXx+s44G3VrB43Xa6tEvjsmP7ctmEfHKzvR7COfCrklwbZWa8u2ILD7y1gpmfbCQ1KYnJo3px9Qn9ObKn10O4tq256xhelPQSNa9K+neUgUwE/kDQg9sDZva/9cz3ZeApYKyZ+VHfHRRJjB/QlfEDurJicxkPvr2Cp8J6iHH9u3DFhHzOGtbDe5ZzrhExuypJUjJBn89nAkUEfT5PMbOPas2XDfwLSAOubywx+BmDa4rSXeVML1zDo3NWsWbLbnKz07n02L5cOq6vX+7q2pSYdu0ZHvAvMbPHG5lvAnCLmZ0VDv8EwMx+VWu+O4FXgJuAmzwxuFiorDLe+HQTj8xexetLiklJEpOO7skVE/IZk9/Zr2ZyrV6zFCVJ6gBcB/QGZgAvh8M/BBYADSaG8H1rIoaLgGNrreMYIM/M/inppgZimQpMBejbt28jq3XuQMlJ4rSh3TltaHdWbi7j0TmrmF64hhkL1zG0RzaXT8jn/FG9/a5q52i42e3ngK3AbOB0oDNBcc93zWxBowsOGt87y8yuCYcvB8aZ2XfC4STgVeBKM1sp6XX8jMEdRrvKK3huwToenb2Kj9Zvp316Cl8a3ZvLxud7B0Ku1WmuyucB1VceSXoA2Az0NbMdUcZRBORFDPcB1kUMZwPDgdfD0/gewAxJk70C2h0OWWkpTBnXl0vG5jF/TSmPzV7Fk3PX8MjsVYzr34XLxuczcVgP0lK8stq1LQ2dMdTo0rOuLj4bXLCUQlD5fDqwlqDy+VIzW1zP/K/jZwwuzraUlfNU4Roef3c1q7fsolv7NC4em8elx+bTu1NmvMNz7qA1Vw9ulUBZ9SCQCewKX5uZNXphuKRJwJ0El6tOM7NfSLoNKDSzGbXmfR1PDK6FqKoy3lxazGNzVvPqJxsBOG1oLpeNz+ekwTkkeRPgLsHE9KqkePPE4A63taW7eeLd1Tw5dzWbd5bTt0sWXz22LxcV5NGlXVq8w3MuKp4YnIuB8ooqXly8gcfmrOK9FVtIS0ninKN7ctmEfI7J6+SXvLoWzRODczG2ZMMOHn93Ff+Yt5adeysY2iObL4/pwwXH9KZr+/R4h+fcATwxOHeY7NxbwXML1jK9sIiFa0pJSRKnH5nLVwryOPmIHFK8+Q3XQnhicC4OPt24g6cK1/CPeWspKSsnJzudLx3Tm4sK+jAo1++LcPHlicG5ONpXWcVrn2xiemERry3ZRGWVMSqvExcV9OGcEb3omJka7xBdG+SJwbkWonjHXp5bsJanCotYsnEH6SlJnDWsBxcV9OG4gd1I9ste3WHiicG5FsbM+GDtNp4qLOK5BWvZvqeCnh0zmDyqFxcc09v7rXYx54nBuRZsz75KXv5oI/+YV8SbSzdTWWUM7ZHNeaN6c96oXvTyO6xdDHhicC5BlOzcy78+WM+z89cyb3UpAOMHdOFLx/Th7KN7kJ3h9RGueXhicC4BrSop47kF63hm/lpWbC4jPSWJLwzrwZdG9+bEQd380ld3SDwxOJfAzIz5a0p5Zt5anl+0jtJd++jWPp3JI3vxpdG9Gdarg99l7ZrME4NzrUR5RRWvLdnEM/PWMvOTjeyrNI7o3p5zR/Ri0oieDMxpH+8QXYLwxOBcK1S6q5x/LgrqIwpXbQVgaI9sJh3dk0lH92RQricJVz9PDM61cuu37ebfH2zghQ/W708SQ7pXJ4keDPYe6Fwtnhica0M2bNvDix+u54UPNjB31RbMYHBueyYd3ZMvjujp3ZQ6oAUlBkkTgT8QdNTzgJn9b63p3wSuAyqBncBUM/uooWV6YnCufhu37+HFDzfwrw/WM3dlkCQGVSeJo3tyRPf2XnHdRrWIxCApmaBrzzMJ+n+eC0yJPPBL6mBm28PXk4Fvm9nEhpbricG56GzavocXFwfFTe+t2EKVwcCcdvvrJIb2yPYk0YY0JTGkxDCOccAyM1seBvUkcB6wPzFUJ4VQOyCxyrWca8FyO2RwxYR+XDGhH8U79gZJYtF67nltGf/36jIGdPs8SRzZ05OE+1wsE0NvYE3EcBFwbO2ZJF0H3AikAafFMB7n2qyc7HQuH5/P5ePzKd6xl5fCM4k/vr6Mu19bRv9u7Zh0dA/OHt7T75NwMS1Kugg4y8yuCYcvB8aZ2Xfqmf/ScP6v1TFtKjAVoG/fvmNWrVoVk5ida2tKdu7lpcUbeeGD9cxeXkJlldGvaxYTh/fkzKNyGZXX2VuAbSVaSh3DBOAWMzsrHP4JgJn9qp75k4CtZtaxoeV6HYNzsbGlrHz/mcTsz0qoqDK6tkvj1KG5nHFkd046ohtZabEsZHCx1FLqGOYCgyX1B9YClwCXRs4gabCZLQ0HvwgsxTkXF13apTFlXF+mjOvLtt37eOPTYmZ+vJH/LN7A0+8XkZ6SxImDu3HykFwmDOjKwJx2XuTUSsUsMZhZhaTrgZcILledZmaLJd0GFJrZDOB6SWcA+4CtwAHFSM65w69jZiqTR/Zi8she7KusYu6KLfzno428/NFGXvl4EwC52emMH9CV8QO6MmFgV/p1zfJE0Ur4DW7OuaiZGatKdjF7eQmzPyth9vISinfsBaBHhwzGD+jChIFBsujbxRNFS9Ii6hhixRODcy2HmbF8cxmzPythzvLgsXlnOQC9OmYwPkwSEwZ0Ja9LVpyjbds8MTjn4sLM+Kx45/6ziTnLt7ClLEgUfTpn7k8So/M7e9HTYeaJwTnXIlRVGUs37WT2Z5uZs3wLc1aUULprHwCdslIZ2acTo/I6MapvJ0b16UTndmlxjrj18sTgnGuRqqqMTzftYP7qUhasLmXBmlI+3bSD6sNQv65ZjMzrxNAeHRjaM5uhPbLp0SHDzyyagScG51zC2Lm3gkVFQZJYuKaURUXbWL9tz/7pHTJSGNqjA0N6ZDOkR5AshvTI9v6wm6il3MfgnHONap+ewnEDu3HcwG77x23btY8lG3fwyYbtfLJhB0s27ODZ+WvZsbdi/zy9O2XuTxJBwuhA/27tSEvxvrEPlScG51yL0zErlXH9uzCuf5f948yMtaW7WbJhx/5k8cmG7bzxaTEVVUHJR5Kgd+dM+nVtR37XrPC5Hf27ZdGncxYZqcnx2qSE4onBOZcQJNGnc3CAP/3I7vvHl1dU8VnxTpZs2MHyzWWsKilj5eYyZixYx/Y9FRHvh14dM8nvmkV+13b065pFftcsenfKonfnTDpnpXpdRsgTg3MuoaWlJHFkzw4c2bPDAdNKd5WzsmQXKzeXsbKkjFUlu1hZUsaLH65na3h1VLWstGR6dcqkd6dMencOnztl0qtTJr06ZdCjQwYpyW2jmMoTg3Ou1eqUlcaorDRG5XU6YNq2XftYs3UXRVt3s7Z0N2u37mZt6S7Wlu7mg7Xb9t9/US1Jwd3dvfYniyBhdO+QQc+OGfTomEG3dukktYLWaD0xOOfapI5ZqXTM6sjw3nU36Ly7vJJ124KEsa40eKwt3cO60t0sLCrl3x+uZ19lzas6U5JE9w4ZdO+QTs+OmfToGJxp9AgTR48OGeR2SCc9pWXXdXhicM65OmSmJTMwpz0Dc9rXOb2qyigpK2fj9j2s37aHDdt2s2H/6z18vGE7ry3ZxK7yygPe2yEjhW7t0+naPm3/c9d26XTLTqdbuzS6huM6Z6XRMTP1sPeJ4YnBOecOQlKSyMlOJyc7vd6zDjNj+56K/clj47Y9bNy+h5Kycop37qVk516WbtrJnOV7D6jzqCYFrd12yUrje2ceweSRvWK5WYAnBuecixlJdMxMpWNmKkd0z25w3n2VVWwtK2fzznJKyvZSsrOcrbvK2bprH6W7ytlSVk6XrMPTZIgnBuecawFSk5PI7ZBBboeMeIdC27j2yjnnXNRimhgkTZS0RNIySTfXMf1GSR9JWiRppqT8WMbjnHOucTFLDJKSgXuAs4GjgCmSjqo123ygwMxGAE8Dv45VPM4556ITyzOGccAyM1tuZuXAk8B5kTOY2WtmtiscnAP0iWE8zjnnohDLxNAbWBMxXBSOq8/VwL/rmiBpqqRCSYXFxcXNGKJzzrnaYpkY6rojo87OHyRdBhQAv6lrupndZ2YFZlaQk5PTjCE655yrLZaXqxYBeRHDfYB1tWeSdAbwU+BkM9sbw3icc85FIZZnDHOBwZL6S0oDLgFmRM4g6Rjgz8BkM9sUw1icc85FKaZde0qaBNwJJAPTzOwXkm4DCs1shqRXgKOB9eFbVpvZ5EaWWQysOsiQugGbD/K9icC3L7G15u1rzdsGibF9+WYWVVl8wvX5fCgkFUbb52ki8u1LbK15+1rztkHr2z6/89k551wNnhicc87V0NYSw33xDiDGfPsSW2vevta8bdDKtq9N1TE455xrXFs7Y3DOOdcITwzOOedqaDOJobEmwBORpJWSPpC0QFJhOK6LpJclLQ2fO8c7zmhImiZpk6QPI8bVuS0K3BXuy0WSRscv8ujUs323SFob7r8F4X0/1dN+Em7fEklnxSfq6EnKk/SapI8lLZb03XB8wu/DBrat1ey/A5hZq38Q3GD3GTAASAMWAkfFO65m2K6VQLda434N3By+vhm4Pd5xRrktJwGjgQ8b2xZgEkGDiwLGA+/GO/6D3L5bgJvqmPeo8DuaDvQPv7vJ8d6GRravJzA6fJ0NfBpuR8Lvwwa2rdXsv9qPtnLG0GgT4K3IecDD4euHgfPjGEvUzOxNYEut0fVty3nAIxaYA3SS1PPwRHpw6tm++pwHPGlme81sBbCM4DvcYpnZejObF77eAXxM0Jpywu/DBratPgm3/2prK4mhqU2AJwoD/iPpfUlTw3HdzWw9BF9oIDdu0R26+ralNe3P68OilGkRxX4JvX2S+gHHAO/SyvZhrW2DVrj/oO0khqibAE8wx5vZaIJe8q6TdFK8AzpMWsv+/BMwEBhF0F7Y78LxCbt9ktoDfwe+Z2bbG5q1jnEtehvr2LZWt/+qtZXEEFUT4InGzNaFz5uAZwhOVzdWn5KHz4ncam1929Iq9qeZbTSzSjOrAu7n8+KGhNw+SakEB87Hzewf4ehWsQ/r2rbWtv8itZXE0GgT4IlGUjtJ2dWvgS8AHxJs19fC2b4GPBefCJtFfdsyA7givLJlPLCturgikdQqU7+AYP9BsH2XSEqX1B8YDLx3uONrCkkC/gJ8bGa/j5iU8Puwvm1rTfvvAPGu/T5cD4KrID4luELgp/GOpxm2ZwDBlQ8LgcXV2wR0BWYCS8PnLvGONcrteYLgdHwfwT+uq+vbFoJT9XvCffkBUBDv+A9y+x4N419EcDDpGTH/T8PtWwKcHe/4o9i+EwiKSxYBC8LHpNawDxvYtlaz/2o/vEkM55xzNbSVoiTnnHNR8sTgnHOuBk8MzjnnavDE4JxzrgZPDM4552rwxOBcLZIqI1rMXNCcrfFK6hfZwqpzLVFKvANwrgXabWaj4h2Ec/HiZwzORSns/+J2Se+Fj0Hh+HxJM8PG1GZK6huO7y7pGUkLw8dx4aKSJd0ftu3/H0mZcdso5+rgicG5A2XWKkq6OGLadjMbB9wN3BmOu5ugCekRwOPAXeH4u4A3zGwkQV8Mi8Pxg4F7zGwYUApcGOPtca5J/M5n52qRtNPM2tcxfiVwmpktDxtV22BmXSVtJmgOYV84fr2ZdZNUDPQxs70Ry+gHvGxmg8PhHwOpZvbz2G+Zc9HxMwbnmsbqeV3fPHXZG/G6Eq/rcy2MJwbnmubiiOfZ4et3CFrsBfgqMCt8PRP4FoCkZEkdDleQzh0K/6fi3IEyJS2IGH7RzKovWU2X9C7Bn6op4bgbgGmSfggUA1eF478L3CfpaoIzg28RtLDqXIvmdQzORSmsYygws83xjsW5WPKiJOecczX4GYNzzrka/IzBOedcDZ4YnHPO1eCJwTnnXA2eGJxzztXgicE551wN/z+8GmC8djJ+6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training house polarity model\n",
    "modSen = buildModel()\n",
    "\n",
    "stopCallBack = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "trainedModSen = modSen.fit(normTrainFeatures, train_label, epochs=500, verbose=1, \n",
    "                       validation_split = 0.33,\n",
    "                       callbacks=[stopCallBack])\n",
    "\n",
    "res = pd.DataFrame(trainedModSen.history)\n",
    "res['epoch'] = trainedModSen.epoch\n",
    "\n",
    "# Plotting RMSE and Epoch number\n",
    "x = res['epoch']\n",
    "y = res['rmse']\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Square Error')\n",
    "plt.title('Error in Senate Polarity Training Model by Epoch Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing test features\n",
    "stats_test = train.describe()\n",
    "stats_test = stats.transpose()\n",
    "\n",
    "def normalize_test(val):\n",
    "    return((val - stats_test['mean']) / (stats_test['std']))\n",
    "\n",
    "normTestFeatures = normalize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 - 0s - loss: 0.0771 - rmse: 0.2777\n",
      "[0.07713987761073643, 0.2777407]\n"
     ]
    }
   ],
   "source": [
    "# Testing data\n",
    "score= modSen.evaluate(normTestFeatures, test_label, verbose=2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senate_polarization</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.094881</th>\n",
       "      <td>0.930912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.811150</th>\n",
       "      <td>0.717057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.931738</th>\n",
       "      <td>1.234690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.163673</th>\n",
       "      <td>0.865882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.029827</th>\n",
       "      <td>0.795890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.926856</th>\n",
       "      <td>1.008331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.810126</th>\n",
       "      <td>0.748453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.076631</th>\n",
       "      <td>0.764415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.979597</th>\n",
       "      <td>0.681693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.438798</th>\n",
       "      <td>1.143694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.501871</th>\n",
       "      <td>0.706192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.160493</th>\n",
       "      <td>1.429785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.895747</th>\n",
       "      <td>0.741784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.006952</th>\n",
       "      <td>0.833718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.538937</th>\n",
       "      <td>1.384212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.050808</th>\n",
       "      <td>0.740598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.995688</th>\n",
       "      <td>0.815820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.477664</th>\n",
       "      <td>0.746257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.552401</th>\n",
       "      <td>0.534501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.360744</th>\n",
       "      <td>0.732392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.811150</th>\n",
       "      <td>0.690079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.308323</th>\n",
       "      <td>0.975097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.727014</th>\n",
       "      <td>1.236701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.967341</th>\n",
       "      <td>0.908931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.620524</th>\n",
       "      <td>0.937347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.690967</th>\n",
       "      <td>1.083959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.471283</th>\n",
       "      <td>1.338477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.943840</th>\n",
       "      <td>0.872247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.152750</th>\n",
       "      <td>1.220404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.585636</th>\n",
       "      <td>0.717066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.506856</th>\n",
       "      <td>0.748512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.671317</th>\n",
       "      <td>0.853420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.509508</th>\n",
       "      <td>1.048825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.773426</th>\n",
       "      <td>1.662789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.796911</th>\n",
       "      <td>1.045089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.162972</th>\n",
       "      <td>0.913134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "senate_polarization          \n",
       "1.094881             0.930912\n",
       "0.811150             0.717057\n",
       "0.931738             1.234690\n",
       "1.163673             0.865882\n",
       "1.029827             0.795890\n",
       "0.926856             1.008331\n",
       "0.810126             0.748453\n",
       "1.076631             0.764415\n",
       "0.979597             0.681693\n",
       "1.438798             1.143694\n",
       "0.501871             0.706192\n",
       "1.160493             1.429785\n",
       "0.895747             0.741784\n",
       "1.006952             0.833718\n",
       "1.538937             1.384212\n",
       "1.050808             0.740598\n",
       "0.995688             0.815820\n",
       "0.477664             0.746257\n",
       "0.552401             0.534501\n",
       "0.360744             0.732392\n",
       "0.811150             0.690079\n",
       "1.308323             0.975097\n",
       "1.727014             1.236701\n",
       "0.967341             0.908931\n",
       "0.620524             0.937347\n",
       "0.690967             1.083959\n",
       "1.471283             1.338477\n",
       "0.943840             0.872247\n",
       "1.152750             1.220404\n",
       "0.585636             0.717066\n",
       "0.506856             0.748512\n",
       "0.671317             0.853420\n",
       "0.509508             1.048825\n",
       "1.773426             1.662789\n",
       "1.796911             1.045089\n",
       "1.162972             0.913134"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions for test set \n",
    "pred = modSen.predict(normTestFeatures)\n",
    "results = pd.DataFrame(pred, test_label)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name</th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>percent_private_health_insurance</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>count_bach</th>\n",
       "      <th>total_bach</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "      <th>percent_with_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>78.5</td>\n",
       "      <td>66.9</td>\n",
       "      <td>25.522789</td>\n",
       "      <td>844704</td>\n",
       "      <td>3309607</td>\n",
       "      <td>48123</td>\n",
       "      <td>13667</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>63.3020</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>86.4</td>\n",
       "      <td>63.2</td>\n",
       "      <td>28.798844</td>\n",
       "      <td>138684</td>\n",
       "      <td>481561</td>\n",
       "      <td>73181</td>\n",
       "      <td>6703</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>69.1040</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>86.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>29.356522</td>\n",
       "      <td>1383092</td>\n",
       "      <td>4711362</td>\n",
       "      <td>56581</td>\n",
       "      <td>45322</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>69.3910</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>73.3</td>\n",
       "      <td>61.3</td>\n",
       "      <td>23.407412</td>\n",
       "      <td>470114</td>\n",
       "      <td>2008398</td>\n",
       "      <td>45869</td>\n",
       "      <td>8958</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>63.6665</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>87.9</td>\n",
       "      <td>63.6</td>\n",
       "      <td>33.645021</td>\n",
       "      <td>8976247</td>\n",
       "      <td>26679273</td>\n",
       "      <td>71805</td>\n",
       "      <td>318752</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "      <td>75.7115</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_name  percent_with_internet_subscription  \\\n",
       "0     Alabama                                78.5   \n",
       "1      Alaska                                86.4   \n",
       "2     Arizona                                86.0   \n",
       "3    Arkansas                                73.3   \n",
       "4  California                                87.9   \n",
       "\n",
       "   percent_private_health_insurance  education_25_older_bachelor's_degree  \\\n",
       "0                              66.9                             25.522789   \n",
       "1                              63.2                             28.798844   \n",
       "2                              63.0                             29.356522   \n",
       "3                              61.3                             23.407412   \n",
       "4                              63.6                             33.645021   \n",
       "\n",
       "   count_bach  total_bach  household_median_income  \\\n",
       "0      844704     3309607                    48123   \n",
       "1      138684      481561                    73181   \n",
       "2     1383092     4711362                    56581   \n",
       "3      470114     2008398                    45869   \n",
       "4     8976247    26679273                    71805   \n",
       "\n",
       "   total_moved_to_US_from_abroad  Percent_Very_Religious  \\\n",
       "0                          13667                      54   \n",
       "1                           6703                      28   \n",
       "2                          45322                      31   \n",
       "3                           8958                      50   \n",
       "4                         318752                      29   \n",
       "\n",
       "   Percent_Nonreligious  mass_shootings  believes_climate_change  \\\n",
       "0                    17               9                  63.3020   \n",
       "1                    47               0                  69.1040   \n",
       "2                    39               8                  69.3910   \n",
       "3                    19               3                  63.6665   \n",
       "4                    41              49                  75.7115   \n",
       "\n",
       "   percent_with_disability  \n",
       "0                     16.5  \n",
       "1                     12.6  \n",
       "2                     13.0  \n",
       "3                     18.0  \n",
       "4                     10.6  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing prediction data for 2017\n",
    "predDat2017 = pd.read_csv('./2017Data.csv')\n",
    "predDat2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving state_name for future use\n",
    "stateName = predDat2017.pop('state_name')\n",
    "\n",
    "res2017 = pd.DataFrame(stateName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "      <th>percent_with_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.5</td>\n",
       "      <td>25.522789</td>\n",
       "      <td>48123</td>\n",
       "      <td>13667</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>63.3020</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.4</td>\n",
       "      <td>28.798844</td>\n",
       "      <td>73181</td>\n",
       "      <td>6703</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>69.1040</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.0</td>\n",
       "      <td>29.356522</td>\n",
       "      <td>56581</td>\n",
       "      <td>45322</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>69.3910</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.3</td>\n",
       "      <td>23.407412</td>\n",
       "      <td>45869</td>\n",
       "      <td>8958</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>63.6665</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.9</td>\n",
       "      <td>33.645021</td>\n",
       "      <td>71805</td>\n",
       "      <td>318752</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "      <td>75.7115</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "0                                78.5                             25.522789   \n",
       "1                                86.4                             28.798844   \n",
       "2                                86.0                             29.356522   \n",
       "3                                73.3                             23.407412   \n",
       "4                                87.9                             33.645021   \n",
       "\n",
       "   household_median_income  total_moved_to_US_from_abroad  \\\n",
       "0                    48123                          13667   \n",
       "1                    73181                           6703   \n",
       "2                    56581                          45322   \n",
       "3                    45869                           8958   \n",
       "4                    71805                         318752   \n",
       "\n",
       "   Percent_Very_Religious  Percent_Nonreligious  mass_shootings  \\\n",
       "0                      54                    17               9   \n",
       "1                      28                    47               0   \n",
       "2                      31                    39               8   \n",
       "3                      50                    19               3   \n",
       "4                      29                    41              49   \n",
       "\n",
       "   believes_climate_change  percent_with_disability  \n",
       "0                  63.3020                     16.5  \n",
       "1                  69.1040                     12.6  \n",
       "2                  69.3910                     13.0  \n",
       "3                  63.6665                     18.0  \n",
       "4                  75.7115                     10.6  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing unnecesary columns\n",
    "predDat2017.pop('count_bach')\n",
    "predDat2017.pop('total_bach')\n",
    "predDat2017.pop('percent_private_health_insurance')\n",
    "predDat2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "      <th>percent_with_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.230910</td>\n",
       "      <td>-1.076881</td>\n",
       "      <td>-1.184158</td>\n",
       "      <td>-0.452332</td>\n",
       "      <td>1.861818</td>\n",
       "      <td>-1.722462</td>\n",
       "      <td>0.054419</td>\n",
       "      <td>-1.120732</td>\n",
       "      <td>1.405684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.857618</td>\n",
       "      <td>-0.447650</td>\n",
       "      <td>1.358571</td>\n",
       "      <td>-0.561735</td>\n",
       "      <td>-1.012723</td>\n",
       "      <td>1.281832</td>\n",
       "      <td>-0.790008</td>\n",
       "      <td>0.197327</td>\n",
       "      <td>-0.313942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.751870</td>\n",
       "      <td>-0.340538</td>\n",
       "      <td>-0.325893</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>-0.681045</td>\n",
       "      <td>0.480687</td>\n",
       "      <td>-0.039407</td>\n",
       "      <td>0.262526</td>\n",
       "      <td>-0.137570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.605637</td>\n",
       "      <td>-1.483180</td>\n",
       "      <td>-1.412880</td>\n",
       "      <td>-0.526310</td>\n",
       "      <td>1.419581</td>\n",
       "      <td>-1.522176</td>\n",
       "      <td>-0.508532</td>\n",
       "      <td>-1.037928</td>\n",
       "      <td>2.067079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.254174</td>\n",
       "      <td>0.483153</td>\n",
       "      <td>1.218944</td>\n",
       "      <td>4.340528</td>\n",
       "      <td>-0.902164</td>\n",
       "      <td>0.680974</td>\n",
       "      <td>3.807426</td>\n",
       "      <td>1.698375</td>\n",
       "      <td>-1.195802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "0                           -1.230910                             -1.076881   \n",
       "1                            0.857618                             -0.447650   \n",
       "2                            0.751870                             -0.340538   \n",
       "3                           -2.605637                             -1.483180   \n",
       "4                            1.254174                              0.483153   \n",
       "\n",
       "   household_median_income  total_moved_to_US_from_abroad  \\\n",
       "0                -1.184158                      -0.452332   \n",
       "1                 1.358571                      -0.561735   \n",
       "2                -0.325893                       0.044966   \n",
       "3                -1.412880                      -0.526310   \n",
       "4                 1.218944                       4.340528   \n",
       "\n",
       "   Percent_Very_Religious  Percent_Nonreligious  mass_shootings  \\\n",
       "0                1.861818             -1.722462        0.054419   \n",
       "1               -1.012723              1.281832       -0.790008   \n",
       "2               -0.681045              0.480687       -0.039407   \n",
       "3                1.419581             -1.522176       -0.508532   \n",
       "4               -0.902164              0.680974        3.807426   \n",
       "\n",
       "   believes_climate_change  percent_with_disability  \n",
       "0                -1.120732                 1.405684  \n",
       "1                 0.197327                -0.313942  \n",
       "2                 0.262526                -0.137570  \n",
       "3                -1.037928                 2.067079  \n",
       "4                 1.698375                -1.195802  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing 2017 features \n",
    "stats2017 = predDat2017.describe()\n",
    "stats2017 = stats2017.transpose()\n",
    "\n",
    "def normalize(value):\n",
    "    return((value - stats2017['mean']) / (stats2017['std']))\n",
    "\n",
    "normPred2017Feat = normalize(predDat2017)\n",
    "\n",
    "normPred2017Feat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3241229 ],\n",
       "       [0.88702285],\n",
       "       [0.92496943],\n",
       "       [1.268015  ],\n",
       "       [3.1092749 ],\n",
       "       [1.1198239 ],\n",
       "       [0.9083997 ],\n",
       "       [0.7381938 ],\n",
       "       [1.857679  ],\n",
       "       [0.81153524],\n",
       "       [0.8593426 ],\n",
       "       [1.0203768 ],\n",
       "       [1.8614208 ],\n",
       "       [0.9001738 ],\n",
       "       [0.8931397 ],\n",
       "       [0.9280355 ],\n",
       "       [1.0846874 ],\n",
       "       [1.0119878 ],\n",
       "       [1.5268402 ],\n",
       "       [1.18024   ],\n",
       "       [1.4937065 ],\n",
       "       [0.8101955 ],\n",
       "       [0.7213495 ],\n",
       "       [1.3731109 ],\n",
       "       [0.7602608 ],\n",
       "       [0.7619982 ],\n",
       "       [1.2477144 ],\n",
       "       [0.83476365],\n",
       "       [0.9873085 ],\n",
       "       [1.3115368 ],\n",
       "       [0.91040707],\n",
       "       [1.7073227 ],\n",
       "       [0.8433254 ],\n",
       "       [1.1189576 ],\n",
       "       [1.072735  ],\n",
       "       [1.1733745 ],\n",
       "       [1.0470161 ],\n",
       "       [1.0494759 ],\n",
       "       [0.7400255 ],\n",
       "       [1.0855604 ],\n",
       "       [0.9907613 ],\n",
       "       [0.9941629 ],\n",
       "       [1.4302924 ],\n",
       "       [1.4100585 ],\n",
       "       [1.6856555 ],\n",
       "       [1.0866939 ],\n",
       "       [1.2013572 ],\n",
       "       [1.181124  ],\n",
       "       [0.7360055 ],\n",
       "       [1.0943177 ]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining predictions for 2017 \n",
    "predictions2017 = modSen.predict(normPred2017Feat)\n",
    "predictions2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dataframe with state name and 2017 predictions\n",
    "res2017['senate_polarization'] = predictions2017\n",
    "\n",
    "# Writing results to .csv \n",
    "res2017.to_csv('./2017ResultsSenate.csv', sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
