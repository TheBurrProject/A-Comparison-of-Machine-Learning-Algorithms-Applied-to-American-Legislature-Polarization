{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkNTcFXJkMqg"
   },
   "outputs": [],
   "source": [
    "# Authors: Gabriel Mersy and Isaac Rand\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed for reproducible results\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "4VQ14P39kvlY",
    "outputId": "6337d3bf-bbe6-4c55-b571-de11efea44b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>percent_with_disability</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "      <th>house_polarization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.721508</td>\n",
       "      <td>14.8</td>\n",
       "      <td>42849</td>\n",
       "      <td>15.9</td>\n",
       "      <td>14928</td>\n",
       "      <td>57</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>63.450</td>\n",
       "      <td>0.727397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.882121</td>\n",
       "      <td>18.2</td>\n",
       "      <td>72237</td>\n",
       "      <td>11.1</td>\n",
       "      <td>7081</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>69.588</td>\n",
       "      <td>0.670553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.841647</td>\n",
       "      <td>17.1</td>\n",
       "      <td>48510</td>\n",
       "      <td>12.3</td>\n",
       "      <td>44711</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>68.827</td>\n",
       "      <td>1.582075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.302302</td>\n",
       "      <td>13.4</td>\n",
       "      <td>40511</td>\n",
       "      <td>17.1</td>\n",
       "      <td>9377</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>64.120</td>\n",
       "      <td>0.844130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.750678</td>\n",
       "      <td>19.5</td>\n",
       "      <td>60190</td>\n",
       "      <td>10.6</td>\n",
       "      <td>291448</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>75.241</td>\n",
       "      <td>1.605848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "0                           64.721508                                  14.8   \n",
       "1                           79.882121                                  18.2   \n",
       "2                           74.841647                                  17.1   \n",
       "3                           62.302302                                  13.4   \n",
       "4                           78.750678                                  19.5   \n",
       "\n",
       "   household_median_income  percent_with_disability  \\\n",
       "0                    42849                     15.9   \n",
       "1                    72237                     11.1   \n",
       "2                    48510                     12.3   \n",
       "3                    40511                     17.1   \n",
       "4                    60190                     10.6   \n",
       "\n",
       "   total_moved_to_US_from_abroad  Percent_Very_Religious  \\\n",
       "0                          14928                      57   \n",
       "1                           7081                      38   \n",
       "2                          44711                      36   \n",
       "3                           9377                      51   \n",
       "4                         291448                      34   \n",
       "\n",
       "   Percent_Nonreligious  mass_shootings  believes_climate_change  \\\n",
       "0                    15               1                   63.450   \n",
       "1                    35               0                   69.588   \n",
       "2                    35               1                   68.827   \n",
       "3                    19               0                   64.120   \n",
       "4                    37               4                   75.241   \n",
       "\n",
       "   house_polarization  \n",
       "0            0.727397  \n",
       "1            0.670553  \n",
       "2            1.582075  \n",
       "3            0.844130  \n",
       "4            1.605848  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in data\n",
    "dat = pd.read_csv('./housemerged.csv', sep = ',')\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hlirm9tk_lK"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train, test = sklearn.model_selection.train_test_split(dat, test_size = 0.2, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UW2jmrZwlIxP"
   },
   "outputs": [],
   "source": [
    " # Popping the labels under consideration\n",
    "train_label = train.pop('house_polarization')\n",
    "test_label = test.pop('house_polarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "02MLXft2lMqW",
    "outputId": "ffc7229a-f638-4636-f1ec-d41c7955d113"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>percent_with_disability</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-0.401570</td>\n",
       "      <td>1.705499</td>\n",
       "      <td>1.072746</td>\n",
       "      <td>-1.031287</td>\n",
       "      <td>-0.159451</td>\n",
       "      <td>-0.049657</td>\n",
       "      <td>-0.152386</td>\n",
       "      <td>-0.017854</td>\n",
       "      <td>0.205642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.020013</td>\n",
       "      <td>-0.070592</td>\n",
       "      <td>1.669380</td>\n",
       "      <td>-0.937946</td>\n",
       "      <td>-0.627237</td>\n",
       "      <td>-1.390386</td>\n",
       "      <td>1.988453</td>\n",
       "      <td>-0.532063</td>\n",
       "      <td>0.368208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.024416</td>\n",
       "      <td>-0.146171</td>\n",
       "      <td>-0.179944</td>\n",
       "      <td>-0.704594</td>\n",
       "      <td>3.193395</td>\n",
       "      <td>0.062071</td>\n",
       "      <td>0.115219</td>\n",
       "      <td>-0.017854</td>\n",
       "      <td>0.199704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.381751</td>\n",
       "      <td>0.118354</td>\n",
       "      <td>-0.698908</td>\n",
       "      <td>0.322156</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>1.179345</td>\n",
       "      <td>-1.133604</td>\n",
       "      <td>2.038979</td>\n",
       "      <td>0.205889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-0.793231</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>0.196541</td>\n",
       "      <td>-0.564583</td>\n",
       "      <td>-0.343281</td>\n",
       "      <td>-0.049657</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>-0.532063</td>\n",
       "      <td>0.006208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "153                           -0.401570                              1.705499   \n",
       "47                             1.020013                             -0.070592   \n",
       "84                            -0.024416                             -0.146171   \n",
       "117                            0.381751                              0.118354   \n",
       "178                           -0.793231                              0.269510   \n",
       "\n",
       "     household_median_income  percent_with_disability  \\\n",
       "153                 1.072746                -1.031287   \n",
       "47                  1.669380                -0.937946   \n",
       "84                 -0.179944                -0.704594   \n",
       "117                -0.698908                 0.322156   \n",
       "178                 0.196541                -0.564583   \n",
       "\n",
       "     total_moved_to_US_from_abroad  Percent_Very_Religious  \\\n",
       "153                      -0.159451               -0.049657   \n",
       "47                       -0.627237               -1.390386   \n",
       "84                        3.193395                0.062071   \n",
       "117                       0.131939                1.179345   \n",
       "178                      -0.343281               -0.049657   \n",
       "\n",
       "     Percent_Nonreligious  mass_shootings  believes_climate_change  \n",
       "153             -0.152386       -0.017854                 0.205642  \n",
       "47               1.988453       -0.532063                 0.368208  \n",
       "84               0.115219       -0.017854                 0.199704  \n",
       "117             -1.133604        2.038979                 0.205889  \n",
       "178              0.026017       -0.532063                 0.006208  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing train features\n",
    "stats = train.describe()\n",
    "stats = stats.transpose()\n",
    "\n",
    "def normalize(val):\n",
    "    return((val - stats['mean']) / (stats['std']))\n",
    "\n",
    "normTrainFeatures = normalize(train)\n",
    "\n",
    "normTrainFeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM5ew16alXNv"
   },
   "outputs": [],
   "source": [
    "# Building feed forward neural network model with single hidden layer\n",
    "def buildModel():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(5, input_dim = 9)) \n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss='mse', \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IWywp61RlbW0",
    "outputId": "b50f08e5-c7b7-4f48-9052-ba7c2d1d15d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 48 samples\n",
      "Epoch 1/500\n",
      "96/96 [==============================] - 0s 2ms/sample - loss: 1.0643 - rmse: 1.0317 - val_loss: 1.4040 - val_rmse: 1.1849\n",
      "Epoch 2/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 1.0376 - rmse: 1.0186 - val_loss: 1.3667 - val_rmse: 1.1690\n",
      "Epoch 3/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 1.0086 - rmse: 1.0043 - val_loss: 1.3307 - val_rmse: 1.1536\n",
      "Epoch 4/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.9818 - rmse: 0.9909 - val_loss: 1.2957 - val_rmse: 1.1383\n",
      "Epoch 5/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.9554 - rmse: 0.9775 - val_loss: 1.2623 - val_rmse: 1.1235\n",
      "Epoch 6/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.9305 - rmse: 0.9646 - val_loss: 1.2292 - val_rmse: 1.1087\n",
      "Epoch 7/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.9051 - rmse: 0.9514 - val_loss: 1.1972 - val_rmse: 1.0942\n",
      "Epoch 8/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.8790 - rmse: 0.9375 - val_loss: 1.1667 - val_rmse: 1.0801\n",
      "Epoch 9/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.8576 - rmse: 0.9261 - val_loss: 1.1373 - val_rmse: 1.0665\n",
      "Epoch 10/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.8342 - rmse: 0.9133 - val_loss: 1.1091 - val_rmse: 1.0531\n",
      "Epoch 11/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.8147 - rmse: 0.9026 - val_loss: 1.0816 - val_rmse: 1.0400\n",
      "Epoch 12/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.7924 - rmse: 0.8902 - val_loss: 1.0560 - val_rmse: 1.0276\n",
      "Epoch 13/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.7720 - rmse: 0.8787 - val_loss: 1.0316 - val_rmse: 1.0157\n",
      "Epoch 14/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.7554 - rmse: 0.8691 - val_loss: 1.0078 - val_rmse: 1.0039\n",
      "Epoch 15/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.7368 - rmse: 0.8584 - val_loss: 0.9847 - val_rmse: 0.9923\n",
      "Epoch 16/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.7191 - rmse: 0.8480 - val_loss: 0.9623 - val_rmse: 0.9810\n",
      "Epoch 17/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.7035 - rmse: 0.8387 - val_loss: 0.9405 - val_rmse: 0.9698\n",
      "Epoch 18/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.6863 - rmse: 0.8284 - val_loss: 0.9196 - val_rmse: 0.9590\n",
      "Epoch 19/500\n",
      "96/96 [==============================] - 0s 121us/sample - loss: 0.6713 - rmse: 0.8193 - val_loss: 0.8993 - val_rmse: 0.9483\n",
      "Epoch 20/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.6553 - rmse: 0.8095 - val_loss: 0.8782 - val_rmse: 0.9371\n",
      "Epoch 21/500\n",
      "96/96 [==============================] - 0s 109us/sample - loss: 0.6416 - rmse: 0.8010 - val_loss: 0.8571 - val_rmse: 0.9258\n",
      "Epoch 22/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.6270 - rmse: 0.7918 - val_loss: 0.8371 - val_rmse: 0.9149\n",
      "Epoch 23/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.6137 - rmse: 0.7834 - val_loss: 0.8174 - val_rmse: 0.9041\n",
      "Epoch 24/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.6000 - rmse: 0.7746 - val_loss: 0.7982 - val_rmse: 0.8934\n",
      "Epoch 25/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.5867 - rmse: 0.7660 - val_loss: 0.7792 - val_rmse: 0.8827\n",
      "Epoch 26/500\n",
      "96/96 [==============================] - 0s 128us/sample - loss: 0.5727 - rmse: 0.7568 - val_loss: 0.7611 - val_rmse: 0.8724\n",
      "Epoch 27/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.5606 - rmse: 0.7488 - val_loss: 0.7432 - val_rmse: 0.8621\n",
      "Epoch 28/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.5491 - rmse: 0.7410 - val_loss: 0.7254 - val_rmse: 0.8517\n",
      "Epoch 29/500\n",
      "96/96 [==============================] - 0s 120us/sample - loss: 0.5366 - rmse: 0.7325 - val_loss: 0.7084 - val_rmse: 0.8417\n",
      "Epoch 30/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.5250 - rmse: 0.7246 - val_loss: 0.6923 - val_rmse: 0.8320\n",
      "Epoch 31/500\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 0.5145 - rmse: 0.7173 - val_loss: 0.6764 - val_rmse: 0.8225\n",
      "Epoch 32/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.5050 - rmse: 0.7106 - val_loss: 0.6610 - val_rmse: 0.8130\n",
      "Epoch 33/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.4938 - rmse: 0.7027 - val_loss: 0.6465 - val_rmse: 0.8040\n",
      "Epoch 34/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.4845 - rmse: 0.6961 - val_loss: 0.6324 - val_rmse: 0.7952\n",
      "Epoch 35/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.4749 - rmse: 0.6891 - val_loss: 0.6189 - val_rmse: 0.7867\n",
      "Epoch 36/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.4658 - rmse: 0.6825 - val_loss: 0.6058 - val_rmse: 0.7783\n",
      "Epoch 37/500\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 0.4572 - rmse: 0.6761 - val_loss: 0.5931 - val_rmse: 0.7701\n",
      "Epoch 38/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.4486 - rmse: 0.6698 - val_loss: 0.5811 - val_rmse: 0.7623\n",
      "Epoch 39/500\n",
      "96/96 [==============================] - 0s 118us/sample - loss: 0.4410 - rmse: 0.6640 - val_loss: 0.5693 - val_rmse: 0.7545\n",
      "Epoch 40/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.4327 - rmse: 0.6578 - val_loss: 0.5580 - val_rmse: 0.7470\n",
      "Epoch 41/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.4245 - rmse: 0.6515 - val_loss: 0.5472 - val_rmse: 0.7398\n",
      "Epoch 42/500\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 0.4175 - rmse: 0.6462 - val_loss: 0.5366 - val_rmse: 0.7325\n",
      "Epoch 43/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.4098 - rmse: 0.6401 - val_loss: 0.5265 - val_rmse: 0.7256\n",
      "Epoch 44/500\n",
      "96/96 [==============================] - 0s 117us/sample - loss: 0.4027 - rmse: 0.6346 - val_loss: 0.5165 - val_rmse: 0.7187\n",
      "Epoch 45/500\n",
      "96/96 [==============================] - 0s 85us/sample - loss: 0.3958 - rmse: 0.6292 - val_loss: 0.5061 - val_rmse: 0.7114\n",
      "Epoch 46/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.3891 - rmse: 0.6238 - val_loss: 0.4957 - val_rmse: 0.7040\n",
      "Epoch 47/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.3823 - rmse: 0.6183 - val_loss: 0.4857 - val_rmse: 0.6969\n",
      "Epoch 48/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.3758 - rmse: 0.6130 - val_loss: 0.4759 - val_rmse: 0.6898\n",
      "Epoch 49/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.3693 - rmse: 0.6077 - val_loss: 0.4664 - val_rmse: 0.6830\n",
      "Epoch 50/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.3636 - rmse: 0.6030 - val_loss: 0.4573 - val_rmse: 0.6763\n",
      "Epoch 51/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.3578 - rmse: 0.5982 - val_loss: 0.4485 - val_rmse: 0.6697\n",
      "Epoch 52/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.3523 - rmse: 0.5936 - val_loss: 0.4401 - val_rmse: 0.6634\n",
      "Epoch 53/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.3462 - rmse: 0.5884 - val_loss: 0.4323 - val_rmse: 0.6575\n",
      "Epoch 54/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.3417 - rmse: 0.5846 - val_loss: 0.4243 - val_rmse: 0.6514\n",
      "Epoch 55/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.3365 - rmse: 0.5800 - val_loss: 0.4169 - val_rmse: 0.6457\n",
      "Epoch 56/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.3323 - rmse: 0.5764 - val_loss: 0.4097 - val_rmse: 0.6401\n",
      "Epoch 57/500\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 0.3277 - rmse: 0.5724 - val_loss: 0.4030 - val_rmse: 0.6348\n",
      "Epoch 58/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.3232 - rmse: 0.5685 - val_loss: 0.3966 - val_rmse: 0.6298\n",
      "Epoch 59/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.3192 - rmse: 0.5650 - val_loss: 0.3903 - val_rmse: 0.6248\n",
      "Epoch 60/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.3152 - rmse: 0.5614 - val_loss: 0.3846 - val_rmse: 0.6202\n",
      "Epoch 61/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.3113 - rmse: 0.5579 - val_loss: 0.3790 - val_rmse: 0.6156\n",
      "Epoch 62/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.3074 - rmse: 0.5544 - val_loss: 0.3737 - val_rmse: 0.6113\n",
      "Epoch 63/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.3036 - rmse: 0.5510 - val_loss: 0.3686 - val_rmse: 0.6072\n",
      "Epoch 64/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.3001 - rmse: 0.5478 - val_loss: 0.3638 - val_rmse: 0.6031\n",
      "Epoch 65/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.2968 - rmse: 0.5448 - val_loss: 0.3590 - val_rmse: 0.5992\n",
      "Epoch 66/500\n",
      "96/96 [==============================] - 0s 122us/sample - loss: 0.2933 - rmse: 0.5416 - val_loss: 0.3545 - val_rmse: 0.5954\n",
      "Epoch 67/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.2901 - rmse: 0.5386 - val_loss: 0.3501 - val_rmse: 0.5917\n",
      "Epoch 68/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.2870 - rmse: 0.5357 - val_loss: 0.3459 - val_rmse: 0.5882\n",
      "Epoch 69/500\n",
      "96/96 [==============================] - 0s 118us/sample - loss: 0.2837 - rmse: 0.5326 - val_loss: 0.3420 - val_rmse: 0.5848\n",
      "Epoch 70/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.2810 - rmse: 0.5301 - val_loss: 0.3381 - val_rmse: 0.5815\n",
      "Epoch 71/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.2780 - rmse: 0.5273 - val_loss: 0.3344 - val_rmse: 0.5783\n",
      "Epoch 72/500\n",
      "96/96 [==============================] - 0s 139us/sample - loss: 0.2748 - rmse: 0.5242 - val_loss: 0.3311 - val_rmse: 0.5754\n",
      "Epoch 73/500\n",
      "96/96 [==============================] - 0s 162us/sample - loss: 0.2721 - rmse: 0.5216 - val_loss: 0.3279 - val_rmse: 0.5726\n",
      "Epoch 74/500\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 0.2695 - rmse: 0.5191 - val_loss: 0.3248 - val_rmse: 0.5699\n",
      "Epoch 75/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.2666 - rmse: 0.5163 - val_loss: 0.3218 - val_rmse: 0.5673\n",
      "Epoch 76/500\n",
      "96/96 [==============================] - 0s 121us/sample - loss: 0.2641 - rmse: 0.5139 - val_loss: 0.3189 - val_rmse: 0.5647\n",
      "Epoch 77/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.2615 - rmse: 0.5114 - val_loss: 0.3162 - val_rmse: 0.5623\n",
      "Epoch 78/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.2588 - rmse: 0.5087 - val_loss: 0.3136 - val_rmse: 0.5600\n",
      "Epoch 79/500\n",
      "96/96 [==============================] - 0s 111us/sample - loss: 0.2564 - rmse: 0.5064 - val_loss: 0.3112 - val_rmse: 0.5578\n",
      "Epoch 80/500\n",
      "96/96 [==============================] - 0s 116us/sample - loss: 0.2540 - rmse: 0.5039 - val_loss: 0.3085 - val_rmse: 0.5554\n",
      "Epoch 81/500\n",
      "96/96 [==============================] - 0s 135us/sample - loss: 0.2516 - rmse: 0.5016 - val_loss: 0.3058 - val_rmse: 0.5530\n",
      "Epoch 82/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.2492 - rmse: 0.4992 - val_loss: 0.3032 - val_rmse: 0.5507\n",
      "Epoch 83/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.2468 - rmse: 0.4968 - val_loss: 0.3009 - val_rmse: 0.5485\n",
      "Epoch 84/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.2443 - rmse: 0.4943 - val_loss: 0.2986 - val_rmse: 0.5465\n",
      "Epoch 85/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.2421 - rmse: 0.4921 - val_loss: 0.2964 - val_rmse: 0.5444\n",
      "Epoch 86/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.2400 - rmse: 0.4899 - val_loss: 0.2941 - val_rmse: 0.5423\n",
      "Epoch 87/500\n",
      "96/96 [==============================] - 0s 122us/sample - loss: 0.2377 - rmse: 0.4875 - val_loss: 0.2920 - val_rmse: 0.5403\n",
      "Epoch 88/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.2353 - rmse: 0.4851 - val_loss: 0.2900 - val_rmse: 0.5385\n",
      "Epoch 89/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.2330 - rmse: 0.4827 - val_loss: 0.2880 - val_rmse: 0.5366\n",
      "Epoch 90/500\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 0.2308 - rmse: 0.4804 - val_loss: 0.2860 - val_rmse: 0.5348\n",
      "Epoch 91/500\n",
      "96/96 [==============================] - 0s 151us/sample - loss: 0.2286 - rmse: 0.4781 - val_loss: 0.2842 - val_rmse: 0.5331\n",
      "Epoch 92/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.2262 - rmse: 0.4757 - val_loss: 0.2824 - val_rmse: 0.5314\n",
      "Epoch 93/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.2242 - rmse: 0.4735 - val_loss: 0.2806 - val_rmse: 0.5298\n",
      "Epoch 94/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.2220 - rmse: 0.4712 - val_loss: 0.2789 - val_rmse: 0.5281\n",
      "Epoch 95/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.2200 - rmse: 0.4690 - val_loss: 0.2771 - val_rmse: 0.5264\n",
      "Epoch 96/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.2178 - rmse: 0.4667 - val_loss: 0.2753 - val_rmse: 0.5247\n",
      "Epoch 97/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.2159 - rmse: 0.4647 - val_loss: 0.2736 - val_rmse: 0.5230\n",
      "Epoch 98/500\n",
      "96/96 [==============================] - 0s 119us/sample - loss: 0.2139 - rmse: 0.4624 - val_loss: 0.2718 - val_rmse: 0.5214\n",
      "Epoch 99/500\n",
      "96/96 [==============================] - 0s 116us/sample - loss: 0.2118 - rmse: 0.4602 - val_loss: 0.2701 - val_rmse: 0.5197\n",
      "Epoch 100/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.2099 - rmse: 0.4582 - val_loss: 0.2683 - val_rmse: 0.5180\n",
      "Epoch 101/500\n",
      "96/96 [==============================] - 0s 130us/sample - loss: 0.2079 - rmse: 0.4560 - val_loss: 0.2666 - val_rmse: 0.5163\n",
      "Epoch 102/500\n",
      "96/96 [==============================] - 0s 112us/sample - loss: 0.2060 - rmse: 0.4539 - val_loss: 0.2649 - val_rmse: 0.5147\n",
      "Epoch 103/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.2041 - rmse: 0.4517 - val_loss: 0.2631 - val_rmse: 0.5130\n",
      "Epoch 104/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.2021 - rmse: 0.4495 - val_loss: 0.2615 - val_rmse: 0.5113\n",
      "Epoch 105/500\n",
      "96/96 [==============================] - 0s 120us/sample - loss: 0.2003 - rmse: 0.4475 - val_loss: 0.2599 - val_rmse: 0.5098\n",
      "Epoch 106/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.1986 - rmse: 0.4456 - val_loss: 0.2582 - val_rmse: 0.5081\n",
      "Epoch 107/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.1967 - rmse: 0.4436 - val_loss: 0.2565 - val_rmse: 0.5065\n",
      "Epoch 108/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.1949 - rmse: 0.4415 - val_loss: 0.2550 - val_rmse: 0.5049\n",
      "Epoch 109/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.1932 - rmse: 0.4395 - val_loss: 0.2533 - val_rmse: 0.5033\n",
      "Epoch 110/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.1914 - rmse: 0.4375 - val_loss: 0.2516 - val_rmse: 0.5016\n",
      "Epoch 111/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.1896 - rmse: 0.4355 - val_loss: 0.2499 - val_rmse: 0.4999\n",
      "Epoch 112/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.1880 - rmse: 0.4335 - val_loss: 0.2482 - val_rmse: 0.4982\n",
      "Epoch 113/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.1862 - rmse: 0.4315 - val_loss: 0.2466 - val_rmse: 0.4966\n",
      "Epoch 114/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.1845 - rmse: 0.4295 - val_loss: 0.2450 - val_rmse: 0.4950\n",
      "Epoch 115/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.1829 - rmse: 0.4277 - val_loss: 0.2434 - val_rmse: 0.4933\n",
      "Epoch 116/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.1814 - rmse: 0.4259 - val_loss: 0.2417 - val_rmse: 0.4917\n",
      "Epoch 117/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.1796 - rmse: 0.4238 - val_loss: 0.2403 - val_rmse: 0.4902\n",
      "Epoch 118/500\n",
      "96/96 [==============================] - 0s 70us/sample - loss: 0.1782 - rmse: 0.4221 - val_loss: 0.2386 - val_rmse: 0.4885\n",
      "Epoch 119/500\n",
      "96/96 [==============================] - 0s 120us/sample - loss: 0.1764 - rmse: 0.4200 - val_loss: 0.2372 - val_rmse: 0.4870\n",
      "Epoch 120/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1750 - rmse: 0.4184 - val_loss: 0.2357 - val_rmse: 0.4855\n",
      "Epoch 121/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.1734 - rmse: 0.4165 - val_loss: 0.2342 - val_rmse: 0.4839\n",
      "Epoch 122/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1718 - rmse: 0.4145 - val_loss: 0.2327 - val_rmse: 0.4824\n",
      "Epoch 123/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.1706 - rmse: 0.4130 - val_loss: 0.2312 - val_rmse: 0.4808\n",
      "Epoch 124/500\n",
      "96/96 [==============================] - 0s 76us/sample - loss: 0.1690 - rmse: 0.4111 - val_loss: 0.2298 - val_rmse: 0.4794\n",
      "Epoch 125/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.1676 - rmse: 0.4093 - val_loss: 0.2284 - val_rmse: 0.4779\n",
      "Epoch 126/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.1660 - rmse: 0.4074 - val_loss: 0.2270 - val_rmse: 0.4764\n",
      "Epoch 127/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.1645 - rmse: 0.4056 - val_loss: 0.2255 - val_rmse: 0.4748\n",
      "Epoch 128/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.1629 - rmse: 0.4037 - val_loss: 0.2240 - val_rmse: 0.4733\n",
      "Epoch 129/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.1615 - rmse: 0.4019 - val_loss: 0.2225 - val_rmse: 0.4717\n",
      "Epoch 130/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.1598 - rmse: 0.3998 - val_loss: 0.2211 - val_rmse: 0.4702\n",
      "Epoch 131/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.1584 - rmse: 0.3980 - val_loss: 0.2196 - val_rmse: 0.4686\n",
      "Epoch 132/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.1570 - rmse: 0.3962 - val_loss: 0.2183 - val_rmse: 0.4672\n",
      "Epoch 133/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.1555 - rmse: 0.3944 - val_loss: 0.2169 - val_rmse: 0.4657\n",
      "Epoch 134/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.1541 - rmse: 0.3925 - val_loss: 0.2154 - val_rmse: 0.4641\n",
      "Epoch 135/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.1526 - rmse: 0.3906 - val_loss: 0.2140 - val_rmse: 0.4626\n",
      "Epoch 136/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.1510 - rmse: 0.3886 - val_loss: 0.2127 - val_rmse: 0.4612\n",
      "Epoch 137/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1496 - rmse: 0.3868 - val_loss: 0.2113 - val_rmse: 0.4596\n",
      "Epoch 138/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.1482 - rmse: 0.3850 - val_loss: 0.2100 - val_rmse: 0.4582\n",
      "Epoch 139/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.1468 - rmse: 0.3832 - val_loss: 0.2086 - val_rmse: 0.4568\n",
      "Epoch 140/500\n",
      "96/96 [==============================] - 0s 108us/sample - loss: 0.1454 - rmse: 0.3813 - val_loss: 0.2074 - val_rmse: 0.4554\n",
      "Epoch 141/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.1441 - rmse: 0.3796 - val_loss: 0.2061 - val_rmse: 0.4540\n",
      "Epoch 142/500\n",
      "96/96 [==============================] - 0s 71us/sample - loss: 0.1427 - rmse: 0.3778 - val_loss: 0.2050 - val_rmse: 0.4527\n",
      "Epoch 143/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1415 - rmse: 0.3762 - val_loss: 0.2037 - val_rmse: 0.4513\n",
      "Epoch 144/500\n",
      "96/96 [==============================] - 0s 70us/sample - loss: 0.1403 - rmse: 0.3745 - val_loss: 0.2024 - val_rmse: 0.4499\n",
      "Epoch 145/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.1389 - rmse: 0.3727 - val_loss: 0.2012 - val_rmse: 0.4485\n",
      "Epoch 146/500\n",
      "96/96 [==============================] - 0s 70us/sample - loss: 0.1376 - rmse: 0.3710 - val_loss: 0.2000 - val_rmse: 0.4472\n",
      "Epoch 147/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.1363 - rmse: 0.3692 - val_loss: 0.1988 - val_rmse: 0.4459\n",
      "Epoch 148/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.1351 - rmse: 0.3676 - val_loss: 0.1976 - val_rmse: 0.4445\n",
      "Epoch 149/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1340 - rmse: 0.3660 - val_loss: 0.1965 - val_rmse: 0.4433\n",
      "Epoch 150/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.1328 - rmse: 0.3644 - val_loss: 0.1953 - val_rmse: 0.4419\n",
      "Epoch 151/500\n",
      "96/96 [==============================] - 0s 69us/sample - loss: 0.1315 - rmse: 0.3627 - val_loss: 0.1941 - val_rmse: 0.4405\n",
      "Epoch 152/500\n",
      "96/96 [==============================] - 0s 110us/sample - loss: 0.1304 - rmse: 0.3611 - val_loss: 0.1929 - val_rmse: 0.4392\n",
      "Epoch 153/500\n",
      "96/96 [==============================] - 0s 121us/sample - loss: 0.1292 - rmse: 0.3594 - val_loss: 0.1917 - val_rmse: 0.4379\n",
      "Epoch 154/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.1280 - rmse: 0.3577 - val_loss: 0.1906 - val_rmse: 0.4365\n",
      "Epoch 155/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.1268 - rmse: 0.3561 - val_loss: 0.1894 - val_rmse: 0.4352\n",
      "Epoch 156/500\n",
      "96/96 [==============================] - 0s 73us/sample - loss: 0.1256 - rmse: 0.3544 - val_loss: 0.1882 - val_rmse: 0.4338\n",
      "Epoch 157/500\n",
      "96/96 [==============================] - 0s 108us/sample - loss: 0.1244 - rmse: 0.3527 - val_loss: 0.1870 - val_rmse: 0.4325\n",
      "Epoch 158/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.1234 - rmse: 0.3513 - val_loss: 0.1859 - val_rmse: 0.4312\n",
      "Epoch 159/500\n",
      "96/96 [==============================] - 0s 75us/sample - loss: 0.1221 - rmse: 0.3495 - val_loss: 0.1849 - val_rmse: 0.4300\n",
      "Epoch 160/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1210 - rmse: 0.3479 - val_loss: 0.1837 - val_rmse: 0.4286\n",
      "Epoch 161/500\n",
      "96/96 [==============================] - 0s 71us/sample - loss: 0.1200 - rmse: 0.3463 - val_loss: 0.1827 - val_rmse: 0.4274\n",
      "Epoch 162/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.1189 - rmse: 0.3448 - val_loss: 0.1816 - val_rmse: 0.4262\n",
      "Epoch 163/500\n",
      "96/96 [==============================] - 0s 73us/sample - loss: 0.1178 - rmse: 0.3432 - val_loss: 0.1805 - val_rmse: 0.4248\n",
      "Epoch 164/500\n",
      "96/96 [==============================] - 0s 121us/sample - loss: 0.1167 - rmse: 0.3416 - val_loss: 0.1796 - val_rmse: 0.4238\n",
      "Epoch 165/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.1155 - rmse: 0.3399 - val_loss: 0.1786 - val_rmse: 0.4226\n",
      "Epoch 166/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.1146 - rmse: 0.3385 - val_loss: 0.1776 - val_rmse: 0.4214\n",
      "Epoch 167/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.1136 - rmse: 0.3370 - val_loss: 0.1767 - val_rmse: 0.4203\n",
      "Epoch 168/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.1126 - rmse: 0.3356 - val_loss: 0.1758 - val_rmse: 0.4192\n",
      "Epoch 169/500\n",
      "96/96 [==============================] - 0s 77us/sample - loss: 0.1115 - rmse: 0.3340 - val_loss: 0.1749 - val_rmse: 0.4182\n",
      "Epoch 170/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.1107 - rmse: 0.3327 - val_loss: 0.1740 - val_rmse: 0.4171\n",
      "Epoch 171/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.1098 - rmse: 0.3314 - val_loss: 0.1731 - val_rmse: 0.4161\n",
      "Epoch 172/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.1088 - rmse: 0.3298 - val_loss: 0.1724 - val_rmse: 0.4152\n",
      "Epoch 173/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1079 - rmse: 0.3285 - val_loss: 0.1715 - val_rmse: 0.4141\n",
      "Epoch 174/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.1070 - rmse: 0.3271 - val_loss: 0.1707 - val_rmse: 0.4131\n",
      "Epoch 175/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.1061 - rmse: 0.3258 - val_loss: 0.1699 - val_rmse: 0.4121\n",
      "Epoch 176/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.1053 - rmse: 0.3245 - val_loss: 0.1692 - val_rmse: 0.4113\n",
      "Epoch 177/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.1043 - rmse: 0.3230 - val_loss: 0.1684 - val_rmse: 0.4104\n",
      "Epoch 178/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.1035 - rmse: 0.3218 - val_loss: 0.1677 - val_rmse: 0.4095\n",
      "Epoch 179/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.1026 - rmse: 0.3204 - val_loss: 0.1670 - val_rmse: 0.4087\n",
      "Epoch 180/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.1018 - rmse: 0.3191 - val_loss: 0.1664 - val_rmse: 0.4079\n",
      "Epoch 181/500\n",
      "96/96 [==============================] - 0s 74us/sample - loss: 0.1011 - rmse: 0.3179 - val_loss: 0.1657 - val_rmse: 0.4071\n",
      "Epoch 182/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.1002 - rmse: 0.3165 - val_loss: 0.1650 - val_rmse: 0.4063\n",
      "Epoch 183/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.0994 - rmse: 0.3153 - val_loss: 0.1644 - val_rmse: 0.4054\n",
      "Epoch 184/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0987 - rmse: 0.3141 - val_loss: 0.1637 - val_rmse: 0.4047\n",
      "Epoch 185/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0978 - rmse: 0.3128 - val_loss: 0.1632 - val_rmse: 0.4039\n",
      "Epoch 186/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0971 - rmse: 0.3116 - val_loss: 0.1625 - val_rmse: 0.4031\n",
      "Epoch 187/500\n",
      "96/96 [==============================] - 0s 114us/sample - loss: 0.0962 - rmse: 0.3101 - val_loss: 0.1618 - val_rmse: 0.4023\n",
      "Epoch 188/500\n",
      "96/96 [==============================] - 0s 107us/sample - loss: 0.0954 - rmse: 0.3089 - val_loss: 0.1612 - val_rmse: 0.4015\n",
      "Epoch 189/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.0946 - rmse: 0.3076 - val_loss: 0.1604 - val_rmse: 0.4006\n",
      "Epoch 190/500\n",
      "96/96 [==============================] - 0s 73us/sample - loss: 0.0939 - rmse: 0.3064 - val_loss: 0.1599 - val_rmse: 0.3998\n",
      "Epoch 191/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.0931 - rmse: 0.3051 - val_loss: 0.1593 - val_rmse: 0.3991\n",
      "Epoch 192/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.0923 - rmse: 0.3038 - val_loss: 0.1586 - val_rmse: 0.3983\n",
      "Epoch 193/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.0916 - rmse: 0.3026 - val_loss: 0.1580 - val_rmse: 0.3975\n",
      "Epoch 194/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0908 - rmse: 0.3013 - val_loss: 0.1575 - val_rmse: 0.3968\n",
      "Epoch 195/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.0901 - rmse: 0.3002 - val_loss: 0.1569 - val_rmse: 0.3961\n",
      "Epoch 196/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.0894 - rmse: 0.2990 - val_loss: 0.1562 - val_rmse: 0.3953\n",
      "Epoch 197/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.0887 - rmse: 0.2978 - val_loss: 0.1557 - val_rmse: 0.3946\n",
      "Epoch 198/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.0880 - rmse: 0.2967 - val_loss: 0.1552 - val_rmse: 0.3939\n",
      "Epoch 199/500\n",
      "96/96 [==============================] - 0s 68us/sample - loss: 0.0873 - rmse: 0.2955 - val_loss: 0.1547 - val_rmse: 0.3933\n",
      "Epoch 200/500\n",
      "96/96 [==============================] - 0s 115us/sample - loss: 0.0868 - rmse: 0.2946 - val_loss: 0.1541 - val_rmse: 0.3926\n",
      "Epoch 201/500\n",
      "96/96 [==============================] - 0s 75us/sample - loss: 0.0860 - rmse: 0.2933 - val_loss: 0.1537 - val_rmse: 0.3920\n",
      "Epoch 202/500\n",
      "96/96 [==============================] - 0s 75us/sample - loss: 0.0854 - rmse: 0.2922 - val_loss: 0.1533 - val_rmse: 0.3915\n",
      "Epoch 203/500\n",
      "96/96 [==============================] - 0s 108us/sample - loss: 0.0848 - rmse: 0.2912 - val_loss: 0.1528 - val_rmse: 0.3909\n",
      "Epoch 204/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.0842 - rmse: 0.2901 - val_loss: 0.1524 - val_rmse: 0.3904\n",
      "Epoch 205/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.0836 - rmse: 0.2891 - val_loss: 0.1520 - val_rmse: 0.3898\n",
      "Epoch 206/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.0830 - rmse: 0.2880 - val_loss: 0.1515 - val_rmse: 0.3892\n",
      "Epoch 207/500\n",
      "96/96 [==============================] - 0s 84us/sample - loss: 0.0824 - rmse: 0.2871 - val_loss: 0.1511 - val_rmse: 0.3887\n",
      "Epoch 208/500\n",
      "96/96 [==============================] - 0s 70us/sample - loss: 0.0819 - rmse: 0.2861 - val_loss: 0.1506 - val_rmse: 0.3881\n",
      "Epoch 209/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.0813 - rmse: 0.2850 - val_loss: 0.1502 - val_rmse: 0.3876\n",
      "Epoch 210/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0806 - rmse: 0.2840 - val_loss: 0.1498 - val_rmse: 0.3871\n",
      "Epoch 211/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.0801 - rmse: 0.2831 - val_loss: 0.1495 - val_rmse: 0.3866\n",
      "Epoch 212/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.0796 - rmse: 0.2822 - val_loss: 0.1491 - val_rmse: 0.3861\n",
      "Epoch 213/500\n",
      "96/96 [==============================] - 0s 131us/sample - loss: 0.0790 - rmse: 0.2811 - val_loss: 0.1487 - val_rmse: 0.3856\n",
      "Epoch 214/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.0785 - rmse: 0.2802 - val_loss: 0.1483 - val_rmse: 0.3852\n",
      "Epoch 215/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.0780 - rmse: 0.2793 - val_loss: 0.1480 - val_rmse: 0.3847\n",
      "Epoch 216/500\n",
      "96/96 [==============================] - 0s 104us/sample - loss: 0.0775 - rmse: 0.2784 - val_loss: 0.1476 - val_rmse: 0.3842\n",
      "Epoch 217/500\n",
      "96/96 [==============================] - 0s 97us/sample - loss: 0.0770 - rmse: 0.2775 - val_loss: 0.1473 - val_rmse: 0.3838\n",
      "Epoch 218/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.0765 - rmse: 0.2766 - val_loss: 0.1469 - val_rmse: 0.3833\n",
      "Epoch 219/500\n",
      "96/96 [==============================] - 0s 100us/sample - loss: 0.0760 - rmse: 0.2757 - val_loss: 0.1466 - val_rmse: 0.3829\n",
      "Epoch 220/500\n",
      "96/96 [==============================] - 0s 103us/sample - loss: 0.0755 - rmse: 0.2747 - val_loss: 0.1464 - val_rmse: 0.3826\n",
      "Epoch 221/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0750 - rmse: 0.2739 - val_loss: 0.1460 - val_rmse: 0.3821\n",
      "Epoch 222/500\n",
      "96/96 [==============================] - 0s 122us/sample - loss: 0.0746 - rmse: 0.2731 - val_loss: 0.1457 - val_rmse: 0.3817\n",
      "Epoch 223/500\n",
      "96/96 [==============================] - 0s 102us/sample - loss: 0.0742 - rmse: 0.2723 - val_loss: 0.1454 - val_rmse: 0.3813\n",
      "Epoch 224/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.0737 - rmse: 0.2716 - val_loss: 0.1450 - val_rmse: 0.3808\n",
      "Epoch 225/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0733 - rmse: 0.2708 - val_loss: 0.1447 - val_rmse: 0.3804\n",
      "Epoch 226/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.0729 - rmse: 0.2700 - val_loss: 0.1445 - val_rmse: 0.3801\n",
      "Epoch 227/500\n",
      "96/96 [==============================] - 0s 76us/sample - loss: 0.0725 - rmse: 0.2692 - val_loss: 0.1442 - val_rmse: 0.3797\n",
      "Epoch 228/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.0721 - rmse: 0.2685 - val_loss: 0.1440 - val_rmse: 0.3795\n",
      "Epoch 229/500\n",
      "96/96 [==============================] - 0s 67us/sample - loss: 0.0717 - rmse: 0.2678 - val_loss: 0.1437 - val_rmse: 0.3791\n",
      "Epoch 230/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.0714 - rmse: 0.2671 - val_loss: 0.1436 - val_rmse: 0.3789\n",
      "Epoch 231/500\n",
      "96/96 [==============================] - 0s 82us/sample - loss: 0.0710 - rmse: 0.2665 - val_loss: 0.1434 - val_rmse: 0.3786\n",
      "Epoch 232/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.0706 - rmse: 0.2657 - val_loss: 0.1432 - val_rmse: 0.3784\n",
      "Epoch 233/500\n",
      "96/96 [==============================] - 0s 91us/sample - loss: 0.0703 - rmse: 0.2651 - val_loss: 0.1429 - val_rmse: 0.3780\n",
      "Epoch 234/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.0698 - rmse: 0.2643 - val_loss: 0.1427 - val_rmse: 0.3777\n",
      "Epoch 235/500\n",
      "96/96 [==============================] - 0s 92us/sample - loss: 0.0695 - rmse: 0.2636 - val_loss: 0.1425 - val_rmse: 0.3775\n",
      "Epoch 236/500\n",
      "96/96 [==============================] - 0s 126us/sample - loss: 0.0691 - rmse: 0.2630 - val_loss: 0.1423 - val_rmse: 0.3773\n",
      "Epoch 237/500\n",
      "96/96 [==============================] - 0s 136us/sample - loss: 0.0688 - rmse: 0.2623 - val_loss: 0.1422 - val_rmse: 0.3771\n",
      "Epoch 238/500\n",
      "96/96 [==============================] - 0s 101us/sample - loss: 0.0684 - rmse: 0.2615 - val_loss: 0.1420 - val_rmse: 0.3769\n",
      "Epoch 239/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.0681 - rmse: 0.2609 - val_loss: 0.1419 - val_rmse: 0.3766\n",
      "Epoch 240/500\n",
      "96/96 [==============================] - 0s 96us/sample - loss: 0.0678 - rmse: 0.2603 - val_loss: 0.1417 - val_rmse: 0.3764\n",
      "Epoch 241/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0674 - rmse: 0.2597 - val_loss: 0.1414 - val_rmse: 0.3761\n",
      "Epoch 242/500\n",
      "96/96 [==============================] - 0s 86us/sample - loss: 0.0671 - rmse: 0.2589 - val_loss: 0.1413 - val_rmse: 0.3759\n",
      "Epoch 243/500\n",
      "96/96 [==============================] - 0s 71us/sample - loss: 0.0667 - rmse: 0.2584 - val_loss: 0.1412 - val_rmse: 0.3757\n",
      "Epoch 244/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.0665 - rmse: 0.2578 - val_loss: 0.1410 - val_rmse: 0.3756\n",
      "Epoch 245/500\n",
      "96/96 [==============================] - 0s 90us/sample - loss: 0.0661 - rmse: 0.2571 - val_loss: 0.1409 - val_rmse: 0.3754\n",
      "Epoch 246/500\n",
      "96/96 [==============================] - 0s 120us/sample - loss: 0.0658 - rmse: 0.2565 - val_loss: 0.1409 - val_rmse: 0.3753\n",
      "Epoch 247/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.0655 - rmse: 0.2559 - val_loss: 0.1407 - val_rmse: 0.3751\n",
      "Epoch 248/500\n",
      "96/96 [==============================] - 0s 87us/sample - loss: 0.0651 - rmse: 0.2552 - val_loss: 0.1406 - val_rmse: 0.3749\n",
      "Epoch 249/500\n",
      "96/96 [==============================] - 0s 65us/sample - loss: 0.0648 - rmse: 0.2546 - val_loss: 0.1405 - val_rmse: 0.3748\n",
      "Epoch 250/500\n",
      "96/96 [==============================] - 0s 105us/sample - loss: 0.0645 - rmse: 0.2540 - val_loss: 0.1404 - val_rmse: 0.3748\n",
      "Epoch 251/500\n",
      "96/96 [==============================] - 0s 69us/sample - loss: 0.0642 - rmse: 0.2533 - val_loss: 0.1403 - val_rmse: 0.3746\n",
      "Epoch 252/500\n",
      "96/96 [==============================] - 0s 81us/sample - loss: 0.0639 - rmse: 0.2528 - val_loss: 0.1402 - val_rmse: 0.3745\n",
      "Epoch 253/500\n",
      "96/96 [==============================] - 0s 95us/sample - loss: 0.0636 - rmse: 0.2522 - val_loss: 0.1402 - val_rmse: 0.3745\n",
      "Epoch 254/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.0632 - rmse: 0.2514 - val_loss: 0.1402 - val_rmse: 0.3745\n",
      "Epoch 255/500\n",
      "96/96 [==============================] - 0s 78us/sample - loss: 0.0629 - rmse: 0.2508 - val_loss: 0.1401 - val_rmse: 0.3743\n",
      "Epoch 256/500\n",
      "96/96 [==============================] - 0s 67us/sample - loss: 0.0626 - rmse: 0.2502 - val_loss: 0.1401 - val_rmse: 0.3743\n",
      "Epoch 257/500\n",
      "96/96 [==============================] - 0s 106us/sample - loss: 0.0623 - rmse: 0.2496 - val_loss: 0.1402 - val_rmse: 0.3744\n",
      "Epoch 258/500\n",
      "96/96 [==============================] - 0s 76us/sample - loss: 0.0620 - rmse: 0.2491 - val_loss: 0.1402 - val_rmse: 0.3744\n",
      "Epoch 259/500\n",
      "96/96 [==============================] - 0s 98us/sample - loss: 0.0617 - rmse: 0.2484 - val_loss: 0.1401 - val_rmse: 0.3744\n",
      "Epoch 260/500\n",
      "96/96 [==============================] - 0s 76us/sample - loss: 0.0614 - rmse: 0.2478 - val_loss: 0.1401 - val_rmse: 0.3743\n",
      "Epoch 261/500\n",
      "96/96 [==============================] - 0s 130us/sample - loss: 0.0612 - rmse: 0.2473 - val_loss: 0.1401 - val_rmse: 0.3743\n",
      "Epoch 262/500\n",
      "96/96 [==============================] - 0s 89us/sample - loss: 0.0609 - rmse: 0.2468 - val_loss: 0.1401 - val_rmse: 0.3743\n",
      "Epoch 263/500\n",
      "96/96 [==============================] - 0s 77us/sample - loss: 0.0607 - rmse: 0.2463 - val_loss: 0.1400 - val_rmse: 0.3742\n",
      "Epoch 264/500\n",
      "96/96 [==============================] - 0s 74us/sample - loss: 0.0604 - rmse: 0.2457 - val_loss: 0.1400 - val_rmse: 0.3741\n",
      "Epoch 265/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.0601 - rmse: 0.2452 - val_loss: 0.1400 - val_rmse: 0.3741\n",
      "Epoch 266/500\n",
      "96/96 [==============================] - 0s 94us/sample - loss: 0.0599 - rmse: 0.2448 - val_loss: 0.1400 - val_rmse: 0.3742\n",
      "Epoch 267/500\n",
      "96/96 [==============================] - 0s 74us/sample - loss: 0.0597 - rmse: 0.2443 - val_loss: 0.1400 - val_rmse: 0.3742\n",
      "Epoch 268/500\n",
      "96/96 [==============================] - 0s 83us/sample - loss: 0.0594 - rmse: 0.2437 - val_loss: 0.1400 - val_rmse: 0.3741\n",
      "Epoch 269/500\n",
      "96/96 [==============================] - 0s 93us/sample - loss: 0.0592 - rmse: 0.2433 - val_loss: 0.1399 - val_rmse: 0.3740\n",
      "Epoch 270/500\n",
      "96/96 [==============================] - 0s 79us/sample - loss: 0.0589 - rmse: 0.2428 - val_loss: 0.1399 - val_rmse: 0.3741\n",
      "Epoch 271/500\n",
      "96/96 [==============================] - 0s 88us/sample - loss: 0.0587 - rmse: 0.2424 - val_loss: 0.1399 - val_rmse: 0.3741\n",
      "Epoch 272/500\n",
      "96/96 [==============================] - 0s 80us/sample - loss: 0.0585 - rmse: 0.2419 - val_loss: 0.1400 - val_rmse: 0.3742\n",
      "Epoch 273/500\n",
      "96/96 [==============================] - 0s 76us/sample - loss: 0.0583 - rmse: 0.2414 - val_loss: 0.1400 - val_rmse: 0.3742\n",
      "Epoch 274/500\n",
      "96/96 [==============================] - 0s 99us/sample - loss: 0.0580 - rmse: 0.2409 - val_loss: 0.1400 - val_rmse: 0.3741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error in House Polarity Training Model by Epoch Number')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training house polarity model\n",
    "modHouse = buildModel()\n",
    "\n",
    "stopCallBack = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "trainedModHouse = modHouse.fit(normTrainFeatures, train_label, epochs=500, verbose=1, \n",
    "                       validation_split = 0.33,\n",
    "                       callbacks=[stopCallBack])\n",
    "\n",
    "res = pd.DataFrame(trainedModHouse.history)\n",
    "res['epoch'] = trainedModHouse.epoch\n",
    "\n",
    "# Plotting RMSE and Epoch number\n",
    "x = res['epoch']\n",
    "y = res['rmse']\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Square Error')\n",
    "plt.title('Error in House Polarity Training Model by Epoch Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1bR7l6wlrIy"
   },
   "outputs": [],
   "source": [
    "# Normalizing test features\n",
    "stats_test = train.describe()\n",
    "stats_test = stats.transpose()\n",
    "\n",
    "def normalize_test(val):\n",
    "    return((val - stats_test['mean']) / (stats_test['std']))\n",
    "\n",
    "normTestFeatures = normalize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zFJLi2oqlwKG",
    "outputId": "82711b3b-72bb-4c58-dde9-fa8fc189f9da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 - 0s - loss: 0.1037 - rmse: 0.3220\n",
      "[0.10369262430402967, 0.32201338]\n"
     ]
    }
   ],
   "source": [
    "# Testing data\n",
    "score = modHouse.evaluate(normTestFeatures, test_label, verbose=2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HcmgTvu_ly3c",
    "outputId": "ba4b6a69-e493-40c5-fc23-1299d19fbdc9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house_polarization</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.912521</th>\n",
       "      <td>0.912925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.984257</th>\n",
       "      <td>0.665490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.949808</th>\n",
       "      <td>1.289863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.405306</th>\n",
       "      <td>0.819815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.846402</th>\n",
       "      <td>0.877334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.982943</th>\n",
       "      <td>0.999476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.812063</th>\n",
       "      <td>0.670503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.044024</th>\n",
       "      <td>0.778534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.804316</th>\n",
       "      <td>0.716641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.586086</th>\n",
       "      <td>1.220390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.615525</th>\n",
       "      <td>0.722802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.207021</th>\n",
       "      <td>1.495414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.843072</th>\n",
       "      <td>0.807908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.193623</th>\n",
       "      <td>0.753740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.605848</th>\n",
       "      <td>1.373526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.889816</th>\n",
       "      <td>0.711659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.933907</th>\n",
       "      <td>0.873518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.653304</th>\n",
       "      <td>0.773849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.586880</th>\n",
       "      <td>0.634280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.340249</th>\n",
       "      <td>0.740784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.984257</th>\n",
       "      <td>0.644747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.332578</th>\n",
       "      <td>0.959782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.077331</th>\n",
       "      <td>1.247456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.826335</th>\n",
       "      <td>0.959827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.734257</th>\n",
       "      <td>0.868095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.916657</th>\n",
       "      <td>1.176111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.593955</th>\n",
       "      <td>1.404302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.803521</th>\n",
       "      <td>0.810304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.088020</th>\n",
       "      <td>1.116631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.630238</th>\n",
       "      <td>0.708146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.591140</th>\n",
       "      <td>0.837042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.835097</th>\n",
       "      <td>0.765066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.310796</th>\n",
       "      <td>0.836856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.642647</th>\n",
       "      <td>1.629135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.043365</th>\n",
       "      <td>1.071558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.084609</th>\n",
       "      <td>0.994389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "house_polarization          \n",
       "0.912521            0.912925\n",
       "0.984257            0.665490\n",
       "0.949808            1.289863\n",
       "1.405306            0.819815\n",
       "0.846402            0.877334\n",
       "0.982943            0.999476\n",
       "0.812063            0.670503\n",
       "1.044024            0.778534\n",
       "0.804316            0.716641\n",
       "1.586086            1.220390\n",
       "0.615525            0.722802\n",
       "1.207021            1.495414\n",
       "0.843072            0.807908\n",
       "1.193623            0.753740\n",
       "1.605848            1.373526\n",
       "0.889816            0.711659\n",
       "0.933907            0.873518\n",
       "0.653304            0.773849\n",
       "0.586880            0.634280\n",
       "0.340249            0.740784\n",
       "0.984257            0.644747\n",
       "1.332578            0.959782\n",
       "2.077331            1.247456\n",
       "0.826335            0.959827\n",
       "0.734257            0.868095\n",
       "0.916657            1.176111\n",
       "1.593955            1.404302\n",
       "0.803521            0.810304\n",
       "1.088020            1.116631\n",
       "0.630238            0.708146\n",
       "0.591140            0.837042\n",
       "0.835097            0.765066\n",
       "0.310796            0.836856\n",
       "1.642647            1.629135\n",
       "2.043365            1.071558\n",
       "1.084609            0.994389"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Predictions for test set \n",
    "pred = modHouse.predict(normTestFeatures)\n",
    "results = pd.DataFrame(pred, test_label)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing prediction data for 2017\n",
    "predDat2017 = pd.read_csv('./2017Data.csv')\n",
    "predDat2017.head()\n",
    "\n",
    "# Saving state_name for future use\n",
    "stateName = predDat2017.pop('state_name')\n",
    "\n",
    "res2017 = pd.DataFrame(stateName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "      <th>percent_with_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.5</td>\n",
       "      <td>25.522789</td>\n",
       "      <td>48123</td>\n",
       "      <td>13667</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>63.3020</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.4</td>\n",
       "      <td>28.798844</td>\n",
       "      <td>73181</td>\n",
       "      <td>6703</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>69.1040</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.0</td>\n",
       "      <td>29.356522</td>\n",
       "      <td>56581</td>\n",
       "      <td>45322</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>69.3910</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.3</td>\n",
       "      <td>23.407412</td>\n",
       "      <td>45869</td>\n",
       "      <td>8958</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>63.6665</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.9</td>\n",
       "      <td>33.645021</td>\n",
       "      <td>71805</td>\n",
       "      <td>318752</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "      <td>75.7115</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "0                                78.5                             25.522789   \n",
       "1                                86.4                             28.798844   \n",
       "2                                86.0                             29.356522   \n",
       "3                                73.3                             23.407412   \n",
       "4                                87.9                             33.645021   \n",
       "\n",
       "   household_median_income  total_moved_to_US_from_abroad  \\\n",
       "0                    48123                          13667   \n",
       "1                    73181                           6703   \n",
       "2                    56581                          45322   \n",
       "3                    45869                           8958   \n",
       "4                    71805                         318752   \n",
       "\n",
       "   Percent_Very_Religious  Percent_Nonreligious  mass_shootings  \\\n",
       "0                      54                    17               9   \n",
       "1                      28                    47               0   \n",
       "2                      31                    39               8   \n",
       "3                      50                    19               3   \n",
       "4                      29                    41              49   \n",
       "\n",
       "   believes_climate_change  percent_with_disability  \n",
       "0                  63.3020                     16.5  \n",
       "1                  69.1040                     12.6  \n",
       "2                  69.3910                     13.0  \n",
       "3                  63.6665                     18.0  \n",
       "4                  75.7115                     10.6  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing unnecesary columns\n",
    "predDat2017.pop('count_bach')\n",
    "predDat2017.pop('total_bach')\n",
    "predDat2017.pop('percent_private_health_insurance')\n",
    "predDat2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_with_internet_subscription</th>\n",
       "      <th>education_25_older_bachelor's_degree</th>\n",
       "      <th>household_median_income</th>\n",
       "      <th>total_moved_to_US_from_abroad</th>\n",
       "      <th>Percent_Very_Religious</th>\n",
       "      <th>Percent_Nonreligious</th>\n",
       "      <th>mass_shootings</th>\n",
       "      <th>believes_climate_change</th>\n",
       "      <th>percent_with_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.230910</td>\n",
       "      <td>-1.076881</td>\n",
       "      <td>-1.184158</td>\n",
       "      <td>-0.452332</td>\n",
       "      <td>1.861818</td>\n",
       "      <td>-1.722462</td>\n",
       "      <td>0.054419</td>\n",
       "      <td>-1.120732</td>\n",
       "      <td>1.405684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.857618</td>\n",
       "      <td>-0.447650</td>\n",
       "      <td>1.358571</td>\n",
       "      <td>-0.561735</td>\n",
       "      <td>-1.012723</td>\n",
       "      <td>1.281832</td>\n",
       "      <td>-0.790008</td>\n",
       "      <td>0.197327</td>\n",
       "      <td>-0.313942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.751870</td>\n",
       "      <td>-0.340538</td>\n",
       "      <td>-0.325893</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>-0.681045</td>\n",
       "      <td>0.480687</td>\n",
       "      <td>-0.039407</td>\n",
       "      <td>0.262526</td>\n",
       "      <td>-0.137570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.605637</td>\n",
       "      <td>-1.483180</td>\n",
       "      <td>-1.412880</td>\n",
       "      <td>-0.526310</td>\n",
       "      <td>1.419581</td>\n",
       "      <td>-1.522176</td>\n",
       "      <td>-0.508532</td>\n",
       "      <td>-1.037928</td>\n",
       "      <td>2.067079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.254174</td>\n",
       "      <td>0.483153</td>\n",
       "      <td>1.218944</td>\n",
       "      <td>4.340528</td>\n",
       "      <td>-0.902164</td>\n",
       "      <td>0.680974</td>\n",
       "      <td>3.807426</td>\n",
       "      <td>1.698375</td>\n",
       "      <td>-1.195802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_with_internet_subscription  education_25_older_bachelor's_degree  \\\n",
       "0                           -1.230910                             -1.076881   \n",
       "1                            0.857618                             -0.447650   \n",
       "2                            0.751870                             -0.340538   \n",
       "3                           -2.605637                             -1.483180   \n",
       "4                            1.254174                              0.483153   \n",
       "\n",
       "   household_median_income  total_moved_to_US_from_abroad  \\\n",
       "0                -1.184158                      -0.452332   \n",
       "1                 1.358571                      -0.561735   \n",
       "2                -0.325893                       0.044966   \n",
       "3                -1.412880                      -0.526310   \n",
       "4                 1.218944                       4.340528   \n",
       "\n",
       "   Percent_Very_Religious  Percent_Nonreligious  mass_shootings  \\\n",
       "0                1.861818             -1.722462        0.054419   \n",
       "1               -1.012723              1.281832       -0.790008   \n",
       "2               -0.681045              0.480687       -0.039407   \n",
       "3                1.419581             -1.522176       -0.508532   \n",
       "4               -0.902164              0.680974        3.807426   \n",
       "\n",
       "   believes_climate_change  percent_with_disability  \n",
       "0                -1.120732                 1.405684  \n",
       "1                 0.197327                -0.313942  \n",
       "2                 0.262526                -0.137570  \n",
       "3                -1.037928                 2.067079  \n",
       "4                 1.698375                -1.195802  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing 2017 features \n",
    "stats2017 = predDat2017.describe()\n",
    "stats2017 = stats2017.transpose()\n",
    "\n",
    "def normalize(value):\n",
    "    return((value - stats2017['mean']) / (stats2017['std']))\n",
    "\n",
    "normPred2017Feat = normalize(predDat2017)\n",
    "\n",
    "normPred2017Feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining predictions for 2017 \n",
    "predictions2017 = modHouse.predict(normPred2017Feat)\n",
    "predictions2017\n",
    "\n",
    "# Setting up dataframe with state name and 2017 predictions\n",
    "res2017['house_polarization'] = predictions2017\n",
    "\n",
    "# Writing results to .csv \n",
    "res2017.to_csv('./2017ResultsHouse.csv', sep =',')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "neuralNetHouse.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
